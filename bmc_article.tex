%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}

%%% Load packages
\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
% \usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails
\usepackage{graphicx}
\usepackage{cite}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%From Notebook%%%%%%%%%%%%%%%%%%%%%%%%%%

    \usepackage{graphicx} % Used to insert images
    \usepackage{adjustbox} % Used to constrain images to a maximum size
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    % \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{ulem} % ulem is needed to support strikethroughs (\sout)




    \definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
    \definecolor{darkorange}{rgb}{.71,0.21,0.01}
    \definecolor{darkgreen}{rgb}{.12,.54,.11}
    \definecolor{myteal}{rgb}{.26, .44, .56}
    \definecolor{gray}{gray}{0.45}
    \definecolor{lightgray}{gray}{.95}
    \definecolor{mediumgray}{gray}{.8}
    \definecolor{inputbackground}{rgb}{.95, .95, .85}
    \definecolor{outputbackground}{rgb}{.95, .95, .95}
    \definecolor{traceback}{rgb}{1, .95, .95}
    % ansi colors
    \definecolor{red}{rgb}{.6,0,0}
    \definecolor{green}{rgb}{0,.65,0}
    \definecolor{brown}{rgb}{0.6,0.6,0}
    \definecolor{blue}{rgb}{0,.145,.698}
    \definecolor{purple}{rgb}{.698,.145,.698}
    \definecolor{cyan}{rgb}{0,.698,.698}
    \definecolor{lightgray}{gray}{0.5}

    % bright ansi colors
    \definecolor{darkgray}{gray}{0.25}
    \definecolor{lightred}{rgb}{1.0,0.39,0.28}
    \definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
    \definecolor{lightblue}{rgb}{0.53,0.81,0.92}
    \definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
    \definecolor{lightcyan}{rgb}{0.5,1.0,0.83}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}

\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}




    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    % \hypersetup{
    %   breaklinks=true,  % so long urls are correctly broken across lines
    %   colorlinks=true,
    %   urlcolor=blue,
    %   linkcolor=darkorange,
    %   citecolor=darkgreen,
    %   }
    % Slightly bigger margins than the latex defaults

    % \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \def\includegraphic{}
% \def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs
\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \title{Materials Knowledge Systems in Python - A Data Science Tool for Accelerated Development of Hierarchical Materials}

\title{Materials Knowledge Systems in Python - A Data Science Framework for Accelerated Development of Hierarchical Materials}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1},
   email={davidbrough.net}   % email address
]{\inits{DB}\fnm{David B} \snm{Brough}}
\author[
   addressref={aff2},
   email={http://wd15.github.io/about}
]{\inits{DW}\fnm{Daniel} \snm{Wheeler}}
\author[
   addressref={aff1,aff3},
   corref={aff1},
   email={surya.kalidindi@me.gatech.edu}
]{\inits{SRK}\fnm{Surya R.} \snm{Kalidindi}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgname{School of Computational Science and Engineering, Georgia Institute of Technology}, % university, etc
%   \street{},                     %
  \postcode{30332},                                % post or zip code
  \city{Atlanta},                              % city
  \cny{USA}                                    % country
}
\address[id=aff2]{%
  \orgname{Materials Science and Engineering Division, Material Measurement Laboratory, National Institute of Standards and Technology},
%   \street{D\"{u}sternbrooker Weg 20},20899
  \postcode{20899},
  \city{Gaithersburg},
  \cny{USA}
}
\address[id=aff3]{%
  \orgname{George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology},
%   \street{},                     %
  \postcode{30332},                                % post or zip code
  \city{Atlanta},                              % city
  \cny{USA}                                    % country
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
%\note{Sample of title note}     % note to the article
% \note[id=n1]{Equal contributor} % note, connected to author
\end{artnotes}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

RANDOM IDEAS

TALK ABOUT PYTHON, SCIKIT-LEARN METHODOLOGY BEING A WAY TO UNIFY
DIFFERENT MATERIALS SCIENCE LEARNING AND DATA TRANSFORMS INTO SINGLE
PIPELINES (EITHER ABSTRACT OR CONCLUSION OR ELSEWHERE). EASY TO SHARE
PIPELINES AND EMBED IN OTHER MORE COMPLEX PIPELINES.

TALK ABOUT WHAT PYMKS ADDS ABOVE AND BEYOND SCIKIT-LEARN IN
ABSTRACT. IT ADDS BOTH THE CONVOLUTION LEARNING AND
PERIODICTY/INDEXING ISSUES TO DO WITH MICROSTRUCTURE. ANSWER QUESTION
OF WHY NOT JUST USE SCIKIT-LEARN? NEEDS TO BE CLEAR.

\begin{abstractbox}

\begin{abstract} % abstract

To Do

% The modern advent of information technology has facilitated massive electronic
% collaborations (e-collaborations) that have lead to significant advances in several
% domains including the discover of the Higg's boson, the sequencing of the human genome,
% the Polymath project, the monitoring of species migration and numerous open source software
% projects. E-collaborations allow experts from complementary domains to create close
% collaborations regardless of spatial and temporal distances. E-collaborations
% require cyber-infrastructure that allow members to generated, analyzed, disseminated,
% and consumed information [10]. Open source cyber-infrastructure eliminates
% collaboration hurdles due to software licenses, and can help foster truly
% massive e-collaborations.

% Customized materials design has great potential for impacting virtually all emerging
% technologies, with significant economic consequences [15, 16, 12, 13, 17, 18, 19,
% 20, 21, 22, 23]. However, materials design (including the design of a manufactur-
% ing process route) resulting in the combination of properties desired for a specific
% application is a highly challenging inverse problem due to hierarchical nature of
% materials structure. Material properties are controlled by the hierarchical internal
% structure (over multiple length scales which spans from atomic to macroscopic) as
% well as coupled physical phenomena which can occur at different timescales at each
% of the hierarchical length scales. Characterization of the structure at each of these
% different length scales is often in the form of images which come from different
% experimental/computational techniques resulting in highly heterogeneous data. As a
% result, tailoring the material hierarchical structure to yield desired combinations of
% properties or performance characteristics is enormous difficult.

% There is a critical need for customized analytics that take into account
% the stochastic nature of these data at multiple length scales in order to extract
% relevant and transferable knowledge. Data driven Process-Structure-Property (PSP)
% linkages provide systemic, modular and hierarchical protocols for community engagement
% (i.e., several people making complementary or overlapping contributions to the
% overall curation of materials knowledge). Computationally cheap PSP linkages exact
% valuable materials knowledge from these heterogeneous datasets and provide a
% format that is usable and valuable for design and manufacturing experts.

% The Materials Knowledge Systems in Python project (PyMKS) is the first open
% source materials data science tool that can be used to create high value PSP linkages
% for hierarchical materials that can be leveraged by experts in materials science and
% engineering, manufacturing, and data science communities, and is essential
% component in the cyber-infrastructure needed to realize a modern accelerated
% materials innovation ecosystems. The on going work demonstrates the versatility
% of PyMKS.

% \parttitle{First part title} %if any
% Text for this section.

% \parttitle{Second part title} %if any
% Text for this section.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{TBD}
% \kwd{article}
% \kwd{author}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
\section{Introduction}

% David - the start is a little weak. Logically, you are better off starting with the need for collaborations - i.e., disciplinary siloed efforts versus cross-disciplinary integrated efforts. Although, the virtues of collaborations have been known for a very long time, there were no mechanisms to scale these up dramatically in intensity and numbers. With the advent of modern information technology - the landscape has changed significantly - with the e-collaborations it is now possible to do this. Discuss the main elements of e-collaborations broadly and point out that open source and open access code repositories are a foundational element for an e-collaboration platform. Talk also about the critical role they play in digital capture, curation, and dissemination of workflows - which is central to community adoption and engagement (e.g., e-Science gateways).




% After this broad introduction, you can then narrow down your focus to code repositories and workflows - talk about the current challenges broadly.

% Then move the discussion to materials science domain where this presents a major new opportunity ...

%% May want to integrate this.
Current practices for developing tools and infrastructure used in multiscale materials design, development, and deployment are generally highly localized (sometimes even within a single organization) resulting in major inefficiencyies (duplication of effort, lack of code review, not engaging the right talent for the right task, etc.). Although it is well known that the pace of discovery and innovation  significantly increases with effective collaboration \cite{sawhney2005collaborating, edwards2009open, bayne2008interdisciplinary, boudreau2010open}, scaling such efforts to large heterogeneous communities such as those engaged in materials innovation has been very difficult.

The advent of information technology has facilitated massive electronic collaborations (generally referred to as e-collaborations) that have lead to significant advances in several domains including the discovery of the Higg's boson \cite{aad2012observation}, the sequencing of the human genome \cite{lander2001initial}, the Polymath project \cite{cranshaw2011polymath}, the monitoring of species migration \cite{dickinson2010citizen, hochachka2012data} and numerous open source software projects. E-collaborations allow experts from complementary domains to create highly productive collaborations that transcend geographical, temporal, cultural, and organizational distances. E-collaborations require a supporting cyber-infrastructure that allows team members to generate, analyze, disseminate, access, and consume information at dramatically increased pace and/or quantity \cite{atkins2003revolutionizing}. A key element of this emerging cyber-infrastructure is open source software as it eliminates collaboration hurdles due to software licenses and can help foster truly massive e-collaborations. In other words, even with collaborations involving proprietary data, open source cyber-infrastructure provide a common language that can facilitate e-collaborations with large numbers of team members (could even become a community effort).

Several recent national and international initiatives \cite{anderson2011report, MGIwhite, MGI2014} have been launched with the premise that the adoption and utilization of modern data science and informatics toolsets offers a new opportunity to accelerate dramatically the design and deployment cycle of new advanced materials in commercial products. More specifically, it has been recognized that innovation cyber-ecosystems are needed to allow experts from the materials science and engineering, design and manufacturing, and data science domains to collaborate effectively. The challenge in integrating these traditionally disconnected communities comes from the vast differences in how knowledge is captured, curated, and disseminated in these communities \cite{kalidindi2015data}. More specifically, knowledge systems in the materials field are rarely captured in a digital form. In order to create a modern materials innovation ecosystem, it is imperative that we design, develop, and launch novel collaboration platforms that allow automated distilling of materials knowledge from large amounts of heterogeneous data acquired through customized protocols that are necessarily diverse (elaborated next). It is also imperative that this curated materials knowledge is presented to the design and manufacturing experts in highly accessible (open) formats.

Customized materials design has great potential for impacting virtually all emerging technologies, with significant economic consequences \cite{ward2012materials, allison2006integrated, MGIwhite, MGI2014, allison2011integrated, olson2000designing, national2008integrated, schmitz2012integrative, robinson2013tms, allisonintegrated, TMSfieldstudy}. However, materials design (including the design of a manufacturing process route) resulting in the combination of properties desired for a specific application is a highly challenging inverse problem due to the hierarchical nature of materials internal structure. Material properties are controlled by the hierarchical internal structure (over multiple length scales which spans from atomic to macroscopic) as well as coupled physical phenomena which can occur at different timescales at each of the hierarchical length scales. Characterization of the structure at each of these different length scales is often in the form of images which come from different experimental/computational techniques resulting in highly heterogeneous data. As a result, tailoring the material hierarchical structure to yield desired combinations of properties or performance characteristics is enormously difficult. Figure \ref{fig:length_scales} provides a collection of materials images depicting material structures at different length scales, which are generally acquired using diverse protocols and are captured in equally diverse formats.

\begin{figure}
    \includegraphics[scale=.23]{fig/lengthScales1.png}
    \caption{Heirarchical Materials structure at multiple length scales
      a). Simulated graphene crystalline structure. b). Simulated fivefold icosahedral Al-Ag quasicrystals. c).High resolution electron microscopy image of delamination cracks in h-BN particles subjected to compressive stress in the (0001) planes (within a silicon nitride particulate-reinforced silicon carbide composite. d). Electron diffraction pattern of an icosahedral Zn-Mg-Ho quasicrystal. e). Cross-polarised light image of spherulites in in poly-3-hydroxy butyrate (PHB) f). Cast iron with magnesium induced spheroidised graphite. g). SEM micrograph of a taffeta textile fragment h). Optical microscopy image of a cross-section of an aluminium casting  i). X-ray tomography image of open cell polyurethane foam. Images courtesy of Core-Materials \cite{coreMaterials}.}
  \label{fig:length_scales}
\end{figure}

While the generation (from experiments and computer simulations) and dissemination of datasets consisting of heterogeneous images are necessary elements in a modern materials innovation ecosystem, there is an equally critical need for customized analytics that take into account the stochastic nature of these data at multiple length scales in order to extract high value, transferable, knowledge. Data-driven Process-Structure-Property (PSP) linkages \cite{kalidindi2015hierarchical} provides a systemic, modular, and hierarchical framework for community engagement (i.e., several people making complementary or overlapping contributions to the overall curation of materials knowledge). Computationally cheap PSP linkages also communicate effectively the curated materials knowledge to design and manufacturing experts in highly accessible formats.

The Materials Knowledge Systems in Python project (PyMKS) is the first open source materials data analytics toolkit that can be used to create high value PSP linkages for hierarchical materials in large scale efforts driven and directed by an entire community of users. In this regard, it could be a foundational element of the cyber-infrastructure needed to realize a modern materials innovation ecosystem.

\section{Current Materials Innovation Ecosystem}


% Traditionally the development materials data analytics toolsets and data repositories are highly localized within a few individual groups resulting in major inefficiency (unnecessary duplication of codes, inadequate verification and validation of multiple instantiations of code, not engaging the right talent for the right task, etc.)

Open access materials databases and computational tools are critical components of the cyber-infrastructure needed to curate materials knowledge through effective e-collaborations \cite{bhat2015strategy}. Several materials science open source computational toolsets and databases have emerged in recent years to help realize the vision outlined in the Materials Genome Initiative (MGI) and the Integrated Computational Materials Engineering (ICME) paradigm \cite{ward2012materials, allison2006integrated, MGIwhite, MGI2014, allison2011integrated, olson2000designing, national2008integrated, schmitz2012integrative, robinson2013tms, allisonintegrated, TMSfieldstudy}. Yet, the creation and adoption of a standard materials taxonomy and database schema has not been established due to the unwieldy size of material descriptors and heterogeneous data. Additionally, the coupled physical phenomena that govern material properties is too complex to model all aspects of a material simultaneously using a single computational tool. Consequently, current practices have resulted in the development of computation tools and databases with a narrow focus on specific length/structure scales, material classes, or properties.

NIST Data Gateway contains over 100 free and paid query-able web-based materials databases. These databases contain atomic structure, thermodynamics, kinetics, fundamental physical constants, x-ray spectroscopy, among other features \cite{NISTgateway}. NIST DSpace provides a curation of links to several materials community databases \cite{nist2015dspace}. NIST Materials Data Curation Systems (MDCS) is a general online database that aims to facilitate the capturing, sharing, and transforming of materials data \cite{NISTDCS}. Open Quantum Materials Database (OQMD) is an open source data repository for phase diagrams and electronic ground states computed using density functional theory \cite{saal2013materials}. MatWeb is a database containing materials properties for over 100,000 materials \cite{matweb}.
Atomic FLOW of Materials Discovery (AFLOW) databases millions of materials and properties and hosts computational tools that can be used for atomic simulations \cite{curtarolo2012aflow}. The Materials Project (and the tool pyMatgen) \cite{ong2013python, jain2013commentary} provides open web-based access to computed information on known and predicted materials as well as analysis tools for electronic band structures. The Knowledgebase of Interatomic Models (OpenKIM) hosts open source tools for potentials for molecular simulation of materials \cite{kim2010project}. PRedictive Integrated Structural Materials Science (PRISMS) hosts a suite of ICME tools and datastorage for the metals community focused on microstructure evolution and mechanical properties \cite{prism}.

SPPARKS Kinetic Monte Carlo Simulator (SPPARKS) is a parallel Monte Carlo code for on-lattice and off-lattice models \cite{plimpton2012spparks}. MOOSE is a parallel computational framework for coupled systems of nonlinear equations \cite{gaston2009moose}. Dream3D is a tool used for synthetic microstructures generation, image processing and mesh creation for finite element \cite{groeber2014dream}.

While there exits a sizable number of standard analytics tools \cite{littell2006sas, seabold2010statsmodels, pedregosa2011scikit, albanese2012mlpy, goodfellow2013pylearn2, mckinney2012python, muller2014pystruct, demvsar2004orange, abadi2016tensorflow, van2014scikit}, none of them are tailored to create PSP linkages from materials structure image data and their associated properties. PyMKS aims to seed and nurture an emergent user group in the materials data analytics for establishing homogenization and localization (PSP) linkages by leveraging open source signal processing and machine learning packages in Python. An overview of the PyMKS project accompanied with several examples is presented here. This paper is a call to others interested in participating in this open science activity.


\section{Theoretical Foundations of Materials Knowledge Systems}


Material properties are controlled by their internal structure and the diverse physical phenomena occurring at multiple time and length scales. Generalized composite theories \cite{hill1963elastic, hashin1983analysis} have been developed for hierarchical materials exhibiting well separated length scales in their internal structure. Generally speaking, these theories either address homogenization (i.e., communication of effective properties associated with the structure at a given length scale to a higher length scale) or localization (i.e., spatiotemporal distribution of the imposed macroscale loading conditions to the lower length scale). Consequently, homogenization and localization are the essential building blocks in communicating the salient information in both directions between hierarchical length/structure scales in multiscale materials modeling. It is also pointed out that localization is significantly more difficult to establish, and implicitly provides a solution to homogenization.

The most sophisticated composite theory available today that explicitly accounts for the full details of the material internal structure (also simply referred as microstructure) comes from the use of perturbation theories and Green's functions \cite{brown1955solid, hill1963elastic, kroner1986statistical, kroner1977bounds, kroner1972statistical, etingof1993representations, adams1998mesostructure, fullwood2008strong, torquato2013random, li2006quantitative, milhans2011prediction, adams2013microstructure, garmestani2001statistical}. In this formalism, one usually arrives at a series expansion for both homogenization and localization, where the individual terms in the series involve convolution integrals with kernels based  on  Green's functions. This series expansion was refined and generalized by Adams and co-workers \cite{adams2005finite, adams2013microstructure, binci2008new} through the introduction of the concept of a microstructure function, which conveniently separates each term in the series into a physics-dependent kernel (based on Green's functions) and a microstructure-dependent function (based on the formalism of n-point spatial correlations \cite{etingof1993representations, adams1998mesostructure, fullwood2008strong, torquato2013random, li2006quantitative, milhans2011prediction}).

 Materials Knowledge Systems (MKS) \cite{landi2010multi, kalidindi2010novel, yabansu2014calibrated, al2012multi, kalidindi2011microstructure, gupta2015structure,  cceccen2014data} complements these sophisticated physics-based materials composite theories with a modern data science approach to create a versatile framework for extracting and curating multiscale PSP linkages. More specifically, MKS employs a discretized version of the composite theories mentioned earlier to gain major computational advantages. As a result, highly adaptable and templatable protocols have been created and used successfully to extract robust and versatile homogenization and localization metamodels with impressive accuracy and broad applicability over large microstructure spaces.

The MKS framework is based on the notion of a microstructure function
that describes many attributes and combinations of attributes in
materials systems including phase identifiers, lattice orientation,
chemical composition, defect types and densities, among others. The
microstructure function, $m_j \left(h; s\right)$, represents a
probability distribution for the given local state, $h \in H$, at each
position, $s \in S$, in a given microstructure,
$j$~\cite{niezgoda2013novel, niezgoda2011understanding,
  qidwai2012estimating,niezgoda2010optimized}.  The MKS framework
requires a discretized description of $m_j$, which is denoted here as
$m_j\left[h; s\right]$, where the $\left[\cdot;\cdot\right]$ represent
the discretized space (in contrast to $\left(\cdot;\cdot\right)$,
which defines the continuous space). The ``$;$'' symbol divides
indices in physical space to the left of ``$;$'' from indices in state
space to the right of ``$;$''. In most applications, $S$ is simply
tessellated into voxels on a regular (uniform) grid such that the
position can be given by $s\rightarrow i,j,k$ in three dimensions. On
the other hand $H$ often needs a more sophisticated decomposition than
simple linear elements.

As noted earlier, the local state space in most advanced materials is
likely to demand sophisticated representations. In prior work
\cite{yabansu2014calibrated, yabansu2015representation,
  brough2016microstructure}, it was found that spectral
representations on functions on the local state space offered many
advantages both in compact representation as well as in reducing the
computational cost. In such cases, $h$ indexes the spectral basis
functions employed. The selection of these functions depends on the
nature of local state descriptors. Examples include: (i) the primitive
basis (or indicator functions) used to represent simple tessellation
schemes \cite{landi2010multi, kalidindi2010novel, al2012multi,
  kalidindi2011microstructure, gupta2015structure, cceccen2014data,
  niezgoda2013novel, niezgoda2011understanding, cecen2016versatile},
(ii) generalized spherical harmonics used to represent functions over
the orientation space \cite{yabansu2014calibrated,
  yabansu2015representation}, and (iii) Legendre polynomials used to
represent functions over the concentration space
\cite{brough2016microstructure}.

\subsection{Homogenization}

Comparing different microstructures is quite difficult even after
expressing them in convenient discretized descriptions mainly due to
the lack of a reference point or a natural origin for the index $s$ in
the tessellation of the microstructure volume. Yet the relative
spatial distributions of the local states provide a valuable
representation of the microstructure that can be used effectively to
quantify the microstructure and compare it with other microstructures
in robust and meaningful ways \cite{niezgoda2011understanding,
  niezgoda2010optimized, niezgoda2013novel, cceccen2014data,
  cecen2016versatile}. The lowest order of spatial correlations comes
in the form of 2-point statistics and can be computed as a correlation
of a microstructure function such that
\begin{equation}\label{eq:stats}
    f_j[h, h'; r] = \frac{1}{\Omega_j\left[r\right]} \sum_{s} m_j[h; s] m_j[h'; s +  r]
\end{equation}
\begin{figure}
    \centering
    \includegraphics{fig/stats_micro_example.png}
    \caption{Caption}
    \label{fig:stats}
\end{figure}
where $r$ is a discrete spatial vector within the voxelated domain
specified by $s$, $f_j[h, h'; r]$ is one set of 2-point statistics for
the local stats $h$ and $h'$ and $\Omega_j\left[r\right]$ is a
normalization factor that depends on
$r$~\cite{cecen2016versatile}. The subscript $j$ refers to independent
ensembles of sample microstructures used for analysis (each $j$ could
refer to a separate microstructure image).  The physical
interpretation of the 2-point statistics is explained in
Fig. \ref{fig:stats} with a highly simplified two-phase microstructure
(the two phases are colored white and gray). If the the primitive
basis is used to discretize both the spatial domain and the local
state space then $f_j[h, h'; r]$ can be interpreted as the probability
of finding local states $h$ and $h'$ at the tail and head of the
vector $r$, respectively.

2-Point statistics provide a meaningful representation of the
microstructure, but create an extremely large feature space that often
contains redundant information. Dimensionality reduction can be used
to create low dimensional microstructure descriptors from the sets of
spatial correlations (based on different selections of $h$ and $h'$)
with principal component analysis (PCA). As a convenience for PCA, the
$f_j\left[h, h; r\right]$ indices are expressed as a single index given
by $f_j\left[k\right]$ where $k$ is an unraveling, such that
$k=k\left(h, h', r\right)$ is a unique integer for every combination
of $h$, $h'$ and $r$. The PCA dimensionality reduction can be
mathematically expressed as
\begin{equation} \label{eq:struc}
    f_j\left[k\right] \approx \sum_{k'\in K'} \mu_j\left[k'\right] \phi\left[k',
      k\right] + \overline{f_j}
\end{equation}
In Eq. \ref{eq:struc}, $f_j\left[k\right]$ are the reconstructed
2-point statistics. The $\mu_j[k']$ are low dimensional microstructure
descriptors (the transformed 2-point statistics) or principal
component scores (PC scores). The $\phi\left[k', k\right]$ are the
calibrated principle components (PCs) and the $\overline{f_j}$ are the
mean values of $f_j\left[k\right]$ for each $j$, independent of
$k$. The $k' \in K'$ indices refer to the $\mu_j\left[k'\right]$ in
decreasing order of significance and are independent of $l$, $l'$ and
$r$. The main advantage of this approach is that the
$f_j\left[k\right]$ can be reconstructed to sufficient fidelity with
only a small subset of the $K'$ indices [ADDREFS].

After obtaining the needed dimensionality reduction in the
representation of the material structure, machine learning models can
be used to create homogenization or localization PSP linkages of
interest. As an example, a generic homogenization linkage can be
expressed as
\begin{equation} \label{eq:hom}
    p_j^{\text{eff}} = \mathcal{F}(\mu_j[k'])
\end{equation}
In Eq. \ref{eq:hom}, $p_j^{\text{eff}}$ is the effective materials
response (reflecting an effective property in structure-property
linkages or an evolved low dimensional microstructure descriptor in
process-structure linkages), and $\mathcal{F}$ is a machine learning
function that links $\mu_j[k']$ to $p_j^{\text{eff}}$.

\subsection{Localization}

MKS Localization linkages are significantly more complex than the
homogenization linkages. These are usually expressed in the same
series forms that are derived in the general composite theories, while
employing discretized kernels based on Green's functions
\cite{brown1955solid, hill1963elastic, kroner1986statistical,
  kroner1977bounds, kroner1972statistical, etingof1993representations,
  adams1998mesostructure, fullwood2008strong, torquato2013random,
  li2006quantitative, milhans2011prediction, adams2013microstructure,
  garmestani2001statistical}. Mathematically, the MKS localization
linkages are expressed as
\begin{multline}
    \label{eq:series}
    p_j[s] = \sum_{h; r} \alpha[h; r] m_j[h; s - r] + \sum_{h, h'; r, r'}
    \alpha[h, h'; r, r'] m_j[h; s - r] m_j[h'; s - r'] + ...
\end{multline}
In Eq. \ref{eq:series}, $p_j[s]$ is the spatially resolved (localized)
response field (e.g. a response variable such as stress or strain
rate, or an evolved microstructure function), and $\alpha[h; r]$ are
the Green's function based discretized influence kernels. These
digital kernels are calibrated using regression methods
\cite{al2012multi, kalidindi2010novel, landi2010multi,
  yabansu2014calibrated, yabansu2015representation,
  brough2016microstructure}.

Figs. \ref{fig:hom} and \ref{fig:loc} provide schematic overviews of the MKS homogenization and localization workflows. THE CURRENT IMAGES ARE JUST PLACE HOLDERS. More detailed explanations on the MKS homogenization and localization linkages can be found in prior literature \cite{landi2010multi, kalidindi2010novel, yabansu2014calibrated, al2012multi, kalidindi2011microstructure, gupta2015structure,  cceccen2014data, niezgoda2013novel, niezgoda2011understanding, cecen2016versatile}.

% \begin{equation}
%     \label{eq:loc}
%     p[s] = \sum_{ r, l} \alpha[r, l] m[s -  r, l]
% \end{equation}



\begin{figure}[h!]
  \caption{\csentence{Localization Figure}
     To Do - Place holder}
    \includegraphics[scale=.18]{fig/localization.png}
  \label{fig:loc}
\end{figure}


\begin{figure}[h!]
  \caption{\csentence{Homogenization Figure}
     To Do - Place holder}
    \includegraphics[scale=.18]{fig/homogenization.png}
  \label{fig:hom}
\end{figure}

% Homogenization linkages are created by taking the ensemble average of Eq. \ref{eq:series}. With the ensemble average, the $N^{th}$ term in the series containing the microstructure function $m[s, l]$ becomes the set of $N$-point statistics. It follows that the linear combinations of these $N$-point statistics provide the necessary structure information to predict effect properties, but present an extremely large feature space with redundant information. Low dimensional linearly independent microstructure descriptors are created using the global linear unsupervised dimensionality reduction technique Principal Component Analysis (PCA). These reduced features as well as effective properties serve as inputs and outputs to calibrate machine learning models for homogenization linkages.


\section{Materials Knowledge Systems in Python}

PyMKS is an object-oriented numerical implementation to the MKS theory
developed in the literature~\cite{kalidindi2010novel}. It provides a
high-level, computational efficient framework to implement data
pipelines for classification, cataloging and quantifying materials
structures for PSP relationships. PyMKS is written in Python, a
natural choice for scientific computing due to its ubiquitous use
among the data science community as well as many other favorable
attributes~\cite{perez2011python}. PyMKS is licensed under the
permissive MIT license \cite{MIT} which allows for unrestricted
distribution in commercial and non-commercial systems.

\subsection{Core Functionality}


PyMKS consists of four main components including a set of tools to
compute 2-point statistics, tools for both homogenization and
localization and tools for discretizing the microstructure. In
addition, PyMKS has modules for generating data sets using
conventional numerical simulations and a module for custom
visualization of microstructures. PyMKS is based on the Scikit-learn
data pipelining methdology. This is a high level system for combining
multiple data and machine learning transformations into a single
customizable pipeline with only minimal required code. This approach
makes cross-validation and parameter searches simple to implement and
avoids the complicated book keeping issues associated with training,
testing and validating data pipelines in machine learning.

The starting point for an MKS homogenization analysis is to use
2-point statisics as outlined in Eq.~\ref{eq:stats} and provided in
PyMKS by the \texttt{MKSStructureAnalysis} object, which calculates
the objective low dimensional structure descriptors, $\mu_j[k']$. The
default dimensionality reduction technique is PCA, but any model that
uses the \texttt{transform\_fit} or a ``transformer'' object can be
substituted. After calculating the descriptors, the
\texttt{MKSHomogenizationModel} is used to create linkages between the
$\mu_j[k']$ and the effective material response, $p_j^{\text{eff}}$,
as indicated in Eq.~\ref{eq:hom}. The default machine learning
algorithm is a polynomial regression, but any estimator with the
\texttt{fit} and \texttt{predict} methods can be substitued to create
the linkages between $\mu_j[k']$ and $p_j^{\text{eff}}$.

The \texttt{MKSLocalizationModel} object provides the MKS localization
functionality. It calibrates the first order influence kernels
$\alpha[h; r]$ used to predict local materials responses, $p_j[s]$, as
outlined in Eq.~\ref{eq:series}. The calibration of the influence
kernels is achieved using a variety of linear regression techniques
described in numerous previous studies~\cite{landi2010multi,
  kalidindi2010novel, yabansu2014calibrated,
  brough2016microstructure}. The \texttt{MKSLocalizationModel} object
uses \texttt{fit} and \texttt{predict} methods to follow the standard
interface for a Scikit-learn estimator object.

To use either the homogenization or the localization models in PyMKS,
the microstructure first needs to be represented by a microstructure
function, $m_j\left[h, s\right]$. The \texttt{bases} module in PyMKS
contains four objects for generating the $m_j\left[h, s\right]$ using
a varietly of discretizations~\cite{landi2010multi,
  kalidindi2010novel, yabansu2014calibrated, al2012multi,
  kalidindi2011microstructure, gupta2015structure, cceccen2014data,
  brough2016microstructure}. A \texttt{PrimitiveBasis} object uses
indicator (or hat) functions and is well suited for microstructures
that have discrete local states (e.g., distinct thermodynamic
phases). The \texttt{LegendreBasis} and \texttt{FourierBasis} objects
create spectral representations of microstucture functions defined on
nonperiodic and periodic continuous local state spaces,
respectively. For example, functions over a range of chemical
compositions can be described using \texttt{LegendreBasis}, while
functions over orientations in two-dimensional space can be described
using \texttt{FourierBasis}. Furthermore, \texttt{GSHBasis} creates
compact spectral representations for functions over lattice
orientation space (such as those needed to describe polycrystalline
microstructures)~\cite{ kalidindi2006spectral, shaffer2010building,
  knezevic2010deformation, al2010spectral, duvvuru2007application,
  li2003evolution, li2005texture, li2007processing, li2005processing,
  creuziger2014crystallographic, sundararaghavan2008multi,
  sundararaghavan2007linear}.

PyMKS contains modest data generation tools (in the \texttt{datasets}
module) that are used in both the PyMKS examples and the PyMKS test
suite. The \texttt{MicrostructureGenerator} object creates stochastic
microstructures using digital filters. This assists users in creating
PyMKS workflows even when data is unavailable. PyMKS has objects for
generating sample data from both a spinodal decomposition simulation
(using the \texttt{CahnHilliardSimulation} object) and a linear
elasticity simulation (using the \texttt{ElasticFESimulation}
object). PyMKS comes with custom functions for visualizing
microstructures in elegant ways (in the \texttt{tools} module). These
are are used extensively in the PyMKS example notebooks to minimize
incidental code associated with visualization.

\subsection{Code Design}

ADD MORE ABOUT ABSTRACTIONS HERE. I DON'T THINK THAT WE NEED THIS
SECTION.

\begin{figure}[h!]
  \caption{\csentence{PyMKS Design}
      AGAIN A PLACE HOLDER - An illustration of the design for PyMKS. The four blocks represent the main functionalists in PyMKS. The modules listed on the far left are exposed in the API  while the other plots are private. The arrows indicate module from
      that is either imported.}
    \includegraphics[scale=.5]{fig/pymksOrganization3.png}
  \label{fig:pymks_design}
\end{figure}


\subsection{Underlying Technologies}

PyMKS is built upon the highly optimized Python packages
NumPy~\cite{van2011numpy}, SciPy~\cite{jones2014scipy}, and
Scikit-learn~\cite{pedregosa2011scikit}. NumPy arrays are the primary
data structure used throughout PyMKS and provide the basic vector and
matrix manipulation operations. SciPy's signal processing and
numerical linear algebra functions are used to calibrate models and
generate synthetic data. PyMKS is highly integrated with Scikit-learn
and mimics its simple API in order to leverage from Scikit-learn's
data pipeling methodology for machine learning and data
transformations. In addition, PyMKS uses the Pytest framework nose to
automate execution of the test suite~\cite{pytest2016}.

Optional packages that can be used with PyMKS include Simple Finite
Elements in Python (SfePy)~\cite{cimrman2014sfepy}, the python wrapper
for the FFTW library (pyFFTW)~\cite{frigo1998fftw} and the plotting
package Matplotlib~\cite{hunter2007matplotlib}. SfePy is used to
simulate linear elasticity to create sample response field
data. PyFFTW is a hightly optimized Fast Fourier Transform library
that enhances the efficiency of PyMKS and enables parallel
computations in PyMKS. Matplotlib is used to generate custom
microstructure visualizations.

\subsection{Development Practices}

PyMKS leverages from existing tools, standards and web resources
wherever possible. In particular the developers are an open community
that use GitHub for issue tracking and release management
(see~\url{https://github.com/materialsinnovation/pymks}). Additionally
a Google group is used as a public forum to discuss the project
development, support and announcements
(see~\url{pymks-general@googlegroups.com}). The Travis CI continuous
integration tool is used to automate running the test suite for
branches of the code stored on GitHub. Code standards are maintained
by following the Python PEP8 standards and by reviewing code using
pull requests on GitHub. Detailed administrative guidelines are
outlined in the \texttt{ADMINISTRATA.md} document and potential
developers are encouraged to follow them.

\section{Examples of Homogenization and Localization with PyMKS}

%DAVE - THE PRESENTATION STYLE OF THE EXAMPLES FEELS VERY ADHOC. CAN
%YOU LOOK AT OTHER PAPERS OR BOOKS TO SEE WHAT ARE SOME GOOD STYLES TO
%PRESENT THIS WITHOUT MAKING THIS LOOK TOO ADHOC OR INFORMAL. PERHAPS
%THERE IS A WAY TO DESCRIBE THE CODING PARTS AS BOXED SELF-STANDING
%DESCRIPTIONS AND THE TEXT CAN THEN FOCUS MAINLY ON THE LOGIC AND
%EXPLANATIONS. AS WRITTEN I AM AFRAID THAT THE MATERIALS FOLKS WILL
%NOT BE ABLE TO FOLLOW WHAT YOU ARE SAYING.

    \subsection{Prediction of Effective Stiffness using Homogenization}\label{effective-stiffness-of-composite-material}

    \subsubsection{Data Generation}\label{data-generation}

A set of periodic microstructures and their volume averaged elastic
stress values $\bar{\sigma}_{xx}$ can be generated by importing the
\texttt{make\_elastic\_stress\_random} function from
\texttt{pymks.datasets}. This function has several arguments.
\texttt{n\_samples} is the number of samples that will be generated,
\texttt{size} specifies the dimensions of the microstructures,
\texttt{grain\_size} controls the effective microstructure feature size,
\texttt{elastic\_modulus} and \texttt{poissons\_ratio} are used to
indicate the material property for each of the phases,
\texttt{macro\_strain} is the value of the applied uniaxial strain, and
the \texttt{seed} can be used to change the the random number generator
seed.

Let's go ahead and create 6 different types of microstructures each with
200 samples with dimensions 21 x 21. Each of the 6 samples will have a
different microstructure feature size. The function will return and the
microstructures and their associated volume averaged stress values.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}elastic\PYZus{}stress\PYZus{}random}

\PY{n}{sample\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{200}
\PY{n}{grain\PYZus{}size} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]}
\PY{n}{n\PYZus{}samples} \PY{o}{=} \PY{p}{[}\PY{n}{sample\PYZus{}size}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{6}
\PY{n}{elastic\PYZus{}modulus} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{310}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}
\PY{n}{poissons\PYZus{}ratio} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{0.28}\PY{p}{,} \PY{l+m+mf}{0.3}\PY{p}{)}
\PY{n}{macro\PYZus{}strain} \PY{o}{=} \PY{l+m+mf}{0.001}
        \PY{n}{size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{21}\PY{p}{,} \PY{l+m+mi}{21}\PY{p}{)}

\PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}stress\PYZus{}random}\PY{p}{(}
    \PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,} \PY{n}{grain\PYZus{}size}\PY{o}{=}\PY{n}{grain\PYZus{}size}\PY{p}{,}
    \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,} \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,}
    \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

\end{Verbatim}


    Lets take a look at 3 types the microstructures to get an idea of
what they look like. We can do this by importing
\texttt{draw\_microstructures}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}microstructures}

\PY{n}{X\PYZus{}examples} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{n}{sample\PYZus{}size}\PY{p}{]}
\PY{n}{draw\PYZus{}microstructures}\PY{p}{(}\PY{n}{X\PYZus{}examples}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_8_0.png}
    \end{center}
    { \hspace*{\fill} \\}


    \subsubsection{Calibration of Homogenization Model}\label{modeling-with-mkshomogenizationmodel}

In order to make an instance of the \texttt{MKSHomogenizationModel}, we
need to pass an instance of a basis class. For this particular example,
there are only 2 discrete
phases, so we will use the \texttt{PrimitiveBasis} from \texttt{pymks.bases}.
We only have two phases denoted by 0 and 1, therefore we have two local
states and our domain is 0 to 1. Let's make an instance of the \texttt{MKSHomgenizationModel}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks} \PY{k+kn}{import} \PY{n}{MKSHomogenizationModel}
\PY{k+kn}{from} \PY{n+nn}{pymks.bases} \PY{k+kn}{import} \PY{n}{PrimitiveBasis}

\PY{n}{p\PYZus{}basis} \PY{o}{=} \PY{n}{PrimitiveBasis}\PY{p}{(}\PY{n}{n\PYZus{}states}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{domain}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{model} \PY{o}{=} \PY{n}{MKSHomogenizationModel}\PY{p}{(}\PY{n}{basis}\PY{o}{=}\PY{n}{p\PYZus{}basis}\PY{p}{,} \PY{n}{periodic\PYZus{}axes}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                               \PY{n}{correlations}\PY{o}{=}\PY{p}{[}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\end{Verbatim}

The default machine model used to create the homogenization linkage is polynomial regression.
We need to optimize the number of principal components and the degree of polynomial.
To do this we are going to split the data into test and training
sets. This can be done using the
\texttt{train\_test\_spilt} function from \texttt{sklearn}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{sklearn.cross\PYZus{}validation} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}

\PY{n}{flat\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{size}\PY{p}{,}\PY{p}{)}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
    \PY{n}{X}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{flat\PYZus{}shape}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{,}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
\PY{k}{print}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{k}{print}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(960, 441)
(240, 441)

    \end{Verbatim}

We will use cross validation with the testing data to find the optimal value for our model parameters using \texttt{GridSeachCV} from \texttt{sklearn}. We will pass a dictionary \texttt{params\_to\_tune} with the range of the degree of the
polynomial and the number of principal components
we want to search. Let's vary \texttt{n\_components} from 1 to 11 and \texttt{degree} from
1 to 3.
    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{sklearn.grid\PYZus{}search} \PY{k+kn}{import} \PY{n}{GridSearchCV}

\PY{n}{params\PYZus{}to\PYZus{}tune} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,}
                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{\PYZcb{}}
\PY{n}{fit\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{\PYZcb{}}
\PY{n}{gs} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{params\PYZus{}to\PYZus{}tune}\PY{p}{,}
                  \PY{n}{fit\PYZus{}params}\PY{o}{=}\PY{n}{fit\PYZus{}params}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\end{Verbatim}

    The default \texttt{score} method for the
\texttt{MKSHomogenizationModel} is R-squared.
% value. Let's look at the how the mean R-squared values and their
% standard deviations change, as we varied the number of
% \texttt{n\_components} and \texttt{degree}, using
% \texttt{draw\_gridscores\_matrix} from \texttt{pymks.tools}.

%     \begin{Verbatim}[commandchars=\\\{\}]

% {\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}gridscores\PYZus{}matrix}

%          \PY{n}{draw\PYZus{}gridscores\PYZus{}matrix}\PY{p}{(}\PY{n}{gs}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{score\PYZus{}label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZhy{}Squared}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
%                                 \PY{n}{param\PYZus{}labels}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of Components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Order of Polynomial}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
% \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_27_0.png}
%     \end{center}
%     { \hspace*{\fill} \\}

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Order of Polynomial}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{degree}\PY{p}{)}
\PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of Components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{n\PYZus{}components}\PY{p}{)}
\PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZhy{}squared Value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{gs}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

Order of Polynomial 2
Number of Components 11
R-squared Value 0.999808062591

    \end{Verbatim}

    For the parameter range that we searched, we have found that a model
with 2nd order polynomial and 11 components had the best R-squared
value. Let's look at the results using \texttt{draw\_grid\_scores}, and set the model to have those parameter values.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}gridscores}

\PY{n}{gs\PYZus{}deg\PYZus{}1} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{gs}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}
            \PY{k}{if} \PY{n}{x}\PY{o}{.}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
\PY{n}{gs\PYZus{}deg\PYZus{}2} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{gs}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}
            \PY{k}{if} \PY{n}{x}\PY{o}{.}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
\PY{n}{gs\PYZus{}deg\PYZus{}3} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{gs}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}
            \PY{k}{if} \PY{n}{x}\PY{o}{.}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}

\PY{n}{draw\PYZus{}gridscores}\PY{p}{(}\PY{p}{[}\PY{n}{gs\PYZus{}deg\PYZus{}1}\PY{p}{,}  \PY{n}{gs\PYZus{}deg\PYZus{}2}\PY{p}{,} \PY{n}{gs\PYZus{}deg\PYZus{}3}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{data\PYZus{}labels}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1st Order}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2nd Order}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3rd Order}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                 \PY{n}{param\PYZus{}label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of Components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{score\PYZus{}label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZhy{}Squared}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_31_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}

\end{Verbatim}

Now that we have found optimal values for the parameters \texttt{n\_components} and
\texttt{degree}, lets fit the model with the data.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}

    \subsubsection{Prediction of Effective Stiffness}\label{prediction-using-mkshomogenizationmodel}

    Let's generate some more data to validate our
model. We are going to generate 20 samples of all
six different types of microstructures using the same
\texttt{make\_elastic\_stress\_random} function.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{test\PYZus{}sample\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{20}
\PY{n}{n\PYZus{}samples} \PY{o}{=} \PY{p}{[}\PY{n}{test\PYZus{}sample\PYZus{}size}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{6}
\PY{n}{X\PYZus{}new}\PY{p}{,} \PY{n}{y\PYZus{}new} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}stress\PYZus{}random}\PY{p}{(}
    \PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,} \PY{n}{grain\PYZus{}size}\PY{o}{=}\PY{n}{grain\PYZus{}size}\PY{p}{,}
    \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,} \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,}
    \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\end{Verbatim}

    Now let's predict the stress values for the new microstructures using the \texttt{predict} method.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{y\PYZus{}predict} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}new}\PY{p}{)}

\end{Verbatim}

    We can look to see, if the low-dimensional representation of the new
data is similar to the low-dimensional representation of the data we
used to fit the model using \texttt{draw\_components\_scatter} from
\texttt{pymks.tools}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}components\PYZus{}scatter}

\PY{n}{draw\PYZus{}components\PYZus{}scatter}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{reduced\PYZus{}fit\PYZus{}data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
                         \PY{n}{model}\PY{o}{.}\PY{n}{reduced\PYZus{}predict\PYZus{}data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{,}
                         \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_41_0.png}
    \end{center}
    { \hspace*{\fill} \\}

    The predicted data seems to be reasonably similar to the data we used to
fit the model with. Now let's look at the score value for the predicted
data. We can evaluate our prediction by looking at a
goodness-of-fit plot. We can do this by importing
\texttt{draw\_goodness\_of\_fit} from \texttt{pymks.tools}.

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}goodness\PYZus{}of\PYZus{}fit}

 \PY{n}{fit\PYZus{}data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{y}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{]}\PY{p}{)}
 \PY{n}{pred\PYZus{}data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{y\PYZus{}new}\PY{p}{,} \PY{n}{y\PYZus{}predict}\PY{p}{]}\PY{p}{)}
 \PY{n}{draw\PYZus{}goodness\PYZus{}of\PYZus{}fit}\PY{p}{(}\PY{n}{fit\PYZus{}data}\PY{p}{,} \PY{n}{pred\PYZus{}data}\PY{p}{,}
                      \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_47_0.png}
    \end{center}
    { \hspace*{\fill} \\}

\subsection{Prediction of Local Strain Field with Localization}\label{linear-elasticity-in-2d-for-3-phases}


% \subsubsection{Calibration Data and Delta
% Microstructures}\label{calibration-data-and-delta-microstructures}

% Because we are using distinct phases and the contrast is low we
% only need delta microstructures and their
% strain fields to calibrate the first-order
% influence coefficients.

% Here we use the \texttt{make\_delta\_microstructure} function from
% \texttt{pymks.datasets} to create the delta microstructures needed to
% calibrate the first-order influence coefficients for a two-phase
% microstructure. The \texttt{make\_delta\_microstructure} function uses
% SfePy to generate the data.

%     \begin{Verbatim}[commandchars=\\\{\}]
% {\color{incolor}In [{\color{incolor}1}]:}
%         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
%         \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
% \end{Verbatim}

%     \begin{Verbatim}[commandchars=\\\{\}]
% {\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}microstructures}
%         \PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}delta\PYZus{}microstructures}

%         \PY{n}{n} \PY{o}{=} \PY{l+m+mi}{21}
%         \PY{n}{n\PYZus{}phases} \PY{o}{=} \PY{l+m+mi}{3}
% \end{Verbatim}

%     Let's take a look at a few of the delta microstructures by importing
% \texttt{draw\_microstructures} from \texttt{pymks.tools}.

%     \begin{Verbatim}[commandchars=\\\{\}]
% {\color{incolor}In [{\color{incolor}3}]:} \PY{n}{draw\PYZus{}microstructures}\PY{p}{(}\PY{n}{X\PYZus{}delta}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
% \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_6_0.png}
%     \end{center}
%     { \hspace*{\fill} \\}

%     Using delta microstructures for the calibration of the first-order
% influence coefficients is essentially the same as using a unit
% impulse response to find the kernel of a system in signal processing. Any given delta
% microstructure is composed of only two phases with the center of the microstructure
% and a different phase from the rest. The number
% of delta microstructures that are needed to calibrated the first-order
% coefficients is $N(N-1)$ where $N$ is the number of phases, therefore in
% this example we need 6 delta microstructures.

\subsubsection{Generating Calibration Data}\label{generating-calibration-data}

% The \texttt{make\_elasticFEstrain\_delta} function from
% \texttt{pymks.datasets} provides an easy interface to generate delta
% microstructures and their strain fields, which can then be used for
% calibration of the influence coefficients.

In this example, lets look at a three phase microstructure with elastic
moduli values of 80, 100 and 120 and Poisson's ratio values all equal to
0.3. Let's also set the macroscopic imposed strain equal to 0.02. All of
these parameters used in the simulation must be passed into the
\texttt{make\_elasticFEstrain\_delta}  function from
\texttt{pymks.datasets}. The number of Poisson's
ratio values and elastic moduli values indicates the number of phases.

     \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}microstructures}
\PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}delta\PYZus{}microstructures}

\PY{n}{n} \PY{o}{=} \PY{l+m+mi}{21}
\PY{n}{n\PYZus{}phases} \PY{o}{=} \PY{l+m+mi}{3}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}delta}
\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}microstructure\PYZus{}strain}

\PY{n}{elastic\PYZus{}modulus} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{80}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{120}\PY{p}{)}
\PY{n}{poissons\PYZus{}ratio} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.3}\PY{p}{)}
\PY{n}{macro\PYZus{}strain} \PY{o}{=} \PY{l+m+mf}{0.02}
\PY{n}{size} \PY{o}{=} \PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{n}\PY{p}{)}

\PY{n}{X\PYZus{}delta}\PY{p}{,} \PY{n}{strains\PYZus{}delta} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}delta}\PY{p}{(}
    \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,}
    \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,}
    \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,} \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{)}

\end{Verbatim}

    Let's take a look at one of the delta microstructures and the
$\varepsilon_{xx}$ strain field.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{draw\PYZus{}microstructure\PYZus{}strain}\PY{p}{(}\PY{n}{X\PYZus{}delta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{strains\PYZus{}delta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_11_0.png}
    \end{center}
    { \hspace*{\fill} \\}

\subsubsection{Calibrating Localization Model}\label{calibrating-first-order-influence-coefficients}

Now that we have the delta microstructures and their strain fields, we
will calibrate the influence coefficients by creating an instance of the
\texttt{MKSLocalizatoinModel} class. Because we are going to calibrate
the influence coefficients with microstructures that contain discrete local
states (in this case phases), we can create an
instance of \texttt{PrimitiveBasis} with \texttt{n\_states} equal to 3,
and use it to create an instance of \texttt{MKSLocalizationModel}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks} \PY{k+kn}{import} \PY{n}{MKSLocalizationModel}
\PY{k+kn}{from} \PY{n+nn}{pymks} \PY{k+kn}{import} \PY{n}{PrimitiveBasis}

\PY{n}{p\PYZus{}basis} \PY{o}{=}\PY{n}{PrimitiveBasis}\PY{p}{(}\PY{n}{n\PYZus{}states}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{domain}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\PY{n}{model} \PY{o}{=} \PY{n}{MKSLocalizationModel}\PY{p}{(}\PY{n}{basis}\PY{o}{=}\PY{n}{p\PYZus{}basis}\PY{p}{)}

\end{Verbatim}

    Now, pass the delta microstructures and their strain fields into the
\texttt{fit} method to calibrate the first-order influence coefficients.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}delta}\PY{p}{,} \PY{n}{strains\PYZus{}delta}\PY{p}{)}

\end{Verbatim}

    That's it, the influence coefficient have been calibrated. Let's take a
look at them.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}coeff}

\PY{n}{draw\PYZus{}coeff}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}

\end{Verbatim}
\begin{center}
\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_18_0.png}
\end{center}
% { \hspace*{\fill} \\}

\subsubsection{Prediction of the Strain Field for a Random
Microstructure}\label{predict-of-the-strain-field-for-a-random-microstructure}

Let's now use our instance of the \texttt{MKSLocalizationModel} class
with calibrated influence coefficients to compute the strain field for a
random two-phase microstructure and compare it with the results from a
finite element simulation.

The \texttt{make\_elasticFEstrain\_random} function from
\texttt{pymks.datasets} is an easy way to generate a random
microstructure and its strain field results from finite element
analysis.
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}random}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{101}\PY{p}{)}
\PY{n}{X}\PY{p}{,} \PY{n}{strain} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}random}\PY{p}{(}
    \PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,}
    \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,}
    \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{)}
\PY{n}{draw\PYZus{}microstructure\PYZus{}strain}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{,} \PY{n}{strain}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}

\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_21_0.png}
    \end{center}
    { \hspace*{\fill} \\}


Now, to get the strain field from the \texttt{MKSLocalizationModel},
just pass the same microstructure to the \texttt{predict} method.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{strain\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\end{Verbatim}

    Finally let's compare the results from finite element simulation and the
MKS model.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}strains\PYZus{}compare}

\PY{n}{draw\PYZus{}strains\PYZus{}compare}\PY{p}{(}\PY{n}{strain}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{strain\PYZus{}pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}

\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}

\section{Conclusion}

To Do

% \subsubsection{Resizing the Coefficeints to use on Larger
% Microstructures}\label{resizing-the-coefficeints-to-use-on-larger-microstructures}

% The influence coefficients that were calibrated on a smaller
% microstructure can be used to predict the strain field on a larger
% microstructure though spectral interpolation {[}3{]}, but accuracy of
% the MKS model drops slightly. To demonstrate how this is done, let's
% generate a new larger random microstructure and its strain field.

%     \begin{Verbatim}[commandchars=\\\{\}]

% {\color{incolor}In [{\color{incolor}13}]:} \PY{n}{m} \PY{o}{=} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{n}
%          \PY{n}{size} \PY{o}{=} \PY{p}{(}\PY{n}{m}\PY{p}{,} \PY{n}{m}\PY{p}{)}
%          \PY{k}{print} \PY{n}{size}
%          \PY{n}{X}\PY{p}{,} \PY{n}{strain} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}random}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,}
%                                                   \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,}
%                                                   \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{)}
%          \PY{n}{draw\PYZus{}microstructure\PYZus{}strain}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{,} \PY{n}{strain}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
% \end{Verbatim}

%     \begin{Verbatim}[commandchars=\\\{\}]
% (63, 63)
%     \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_30_1.png}
%     \end{center}
%     { \hspace*{\fill} \\}

%     The influence coefficients that have already been calibrated on a $n$ by
% $n$ delta microstructures, need to be resized to match the shape of the
% new larger $m$ by $m$ microstructure that we want to compute the strain
% field for. This can be done by passing the shape of the new larger
% microstructure into the \texttt{resize\_coeff} method.

%     \begin{Verbatim}[commandchars=\\\{\}]

% {\color{incolor}In [{\color{incolor}14}]:} \PY{n}{model}\PY{o}{.}\PY{n}{resize\PYZus{}coeff}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

% \end{Verbatim}

%     Let's now take a look that ther resized influence coefficients.

%     \begin{Verbatim}[commandchars=\\\{\}]

% {\color{incolor}In [{\color{incolor}15}]:} \PY{n}{draw\PYZus{}coeff}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}
% \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_34_0.png}
%     \end{center}
%     { \hspace*{\fill} \\}

%     Because the coefficients have been resized, they will no longer work for
% our original $n$ by $n$ sized microstructures they were calibrated on,
% but they can now be used on the $m$ by $m$ microstructures. Just like
% before, just pass the microstructure as the argument of the
% \texttt{predict} method to get the strain field.

%     \begin{Verbatim}[commandchars=\\\{\}]
% {\color{incolor}In [{\color{incolor}16}]:} \PY{n}{strain\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

%          \PY{n}{draw\PYZus{}strains\PYZus{}compare}\PY{p}{(}\PY{n}{strain}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{strain\PYZus{}pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
% \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_36_0.png}
%     \end{center}
%     { \hspace*{\fill} \\}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{backmatter}

% \section*{Competing interests}
%   The authors declare that they have no competing interests.

% \section*{Author's contributions}
%     Text for this section \ldots

% \section*{Acknowledgements}
%   Text for this section \ldots
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{bmc_article}      % Bibliography file (usually '*.bib' )
% for author-year bibliography (bmc-mathphys or spbasic)
% a) write to bib file (bmc-mathphys only)
% @settings{label, options="nameyear"}
% b) uncomment next line
%\nocite{label}

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

% \section*{Figures}
%   \begin{figure}[h!]
%   \caption{\csentence{Sample figure title.}
%       A short description of the figure content
%       should go here.}
%       \end{figure}

% \begin{figure}[h!]
%   \caption{\csentence{Sample figure title.}
%       Figure legend text.}
%       \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
% \section*{Tables}
% \begin{table}[h!]
% \caption{Sample table title. This is where the description of the table should go.}
%       \begin{tabular}{cccc}
%         \hline
%           & B1  &B2   & B3\\ \hline
%         A1 & 0.1 & 0.2 & 0.3\\
%         A2 & ... & ..  & .\\
%         A3 & ..  & .   & .\\ \hline
%       \end{tabular}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section*{Additional Files}
%   \subsection*{Additional file 1 --- Sample additional file title}
%     Additional file descriptions text (including details of how to
%     view the file, if it is in a non-standard format or the file extension).  This might
%     refer to a multi-page table or a figure.

%   \subsection*{Additional file 2 --- Sample additional file title}
%     Additional file descriptions text.


% \end{backmatter}
\end{document}

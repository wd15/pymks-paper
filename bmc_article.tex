%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins

%%% loading packages, author definitions

%\documentclass[twocolumn]{bmcart}% uncomment this for twocolumn layout and comment line below
\documentclass{bmcart}

%%% Load packages
\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage[authoryear]{natbib}% uncomment this for author-year bibliography
%\RequirePackage{hyperref}
% \usepackage[utf8]{inputenc} %unicode support
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails
\usepackage{graphicx}
\usepackage{cite}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%From Notebook%%%%%%%%%%%%%%%%%%%%%%%%%%

    \usepackage{graphicx} % Used to insert images
    \usepackage{adjustbox} % Used to constrain images to a maximum size
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    % \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{ulem} % ulem is needed to support strikethroughs (\sout)




    \definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
    \definecolor{darkorange}{rgb}{.71,0.21,0.01}
    \definecolor{darkgreen}{rgb}{.12,.54,.11}
    \definecolor{myteal}{rgb}{.26, .44, .56}
    \definecolor{gray}{gray}{0.45}
    \definecolor{lightgray}{gray}{.95}
    \definecolor{mediumgray}{gray}{.8}
    \definecolor{inputbackground}{rgb}{.95, .95, .85}
    \definecolor{outputbackground}{rgb}{.95, .95, .95}
    \definecolor{traceback}{rgb}{1, .95, .95}
    % ansi colors
    \definecolor{red}{rgb}{.6,0,0}
    \definecolor{green}{rgb}{0,.65,0}
    \definecolor{brown}{rgb}{0.6,0.6,0}
    \definecolor{blue}{rgb}{0,.145,.698}
    \definecolor{purple}{rgb}{.698,.145,.698}
    \definecolor{cyan}{rgb}{0,.698,.698}
    \definecolor{lightgray}{gray}{0.5}

    % bright ansi colors
    \definecolor{darkgray}{gray}{0.25}
    \definecolor{lightred}{rgb}{1.0,0.39,0.28}
    \definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
    \definecolor{lightblue}{rgb}{0.53,0.81,0.92}
    \definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
    \definecolor{lightcyan}{rgb}{0.5,1.0,0.83}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}

\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}




    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    % \hypersetup{
    %   breaklinks=true,  % so long urls are correctly broken across lines
    %   colorlinks=true,
    %   urlcolor=blue,
    %   linkcolor=darkorange,
    %   citecolor=darkgreen,
    %   }
    % Slightly bigger margins than the latex defaults

    % \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \def\includegraphic{}
% \def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs
\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \title{Materials Knowledge Systems in Python - A Data Science Tool for Accelerated Development of Hierarchical Materials}

\title{Materials Knowledge Systems in Python - A Data Science Framework for Accelerated Development of Hierarchical Materials}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1},
   email={davidbrough.net}   % email address
]{\inits{DB}\fnm{David B} \snm{Brough}}
\author[
   addressref={aff2},
   email={http://wd15.github.io/about}
]{\inits{DW}\fnm{Daniel} \snm{Wheeler}}
\author[
   addressref={aff1,aff3},
   corref={aff1},
   email={surya.kalidindi@me.gatech.edu}
]{\inits{SRK}\fnm{Surya R.} \snm{Kalidindi}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{%                           % unique id
  \orgname{School of Computational Science and Engineering, Georgia Institute of Technology}, % university, etc
%   \street{},                     %
  \postcode{30332},                                % post or zip code
  \city{Atlanta},                              % city
  \cny{USA}                                    % country
}
\address[id=aff2]{%
  \orgname{Materials Science and Engineering Division, Material Measurement Laboratory, National Institute of Standards and Technology},
%   \street{D\"{u}sternbrooker Weg 20},20899
  \postcode{20899},
  \city{Gaithersburg},
  \cny{USA}
}
\address[id=aff3]{%
  \orgname{George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology},
%   \street{},                     %
  \postcode{30332},                                % post or zip code
  \city{Atlanta},                              % city
  \cny{USA}                                    % country
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
%\note{Sample of title note}     % note to the article
% \note[id=n1]{Equal contributor} % note, connected to author
\end{artnotes}

\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract} % abstract

To Do

% The modern advent of information technology has facilitated massive electronic
% collaborations (e-collaborations) that have lead to significant advances in several
% domains including the discover of the Higg's boson, the sequencing of the human genome,
% the Polymath project, the monitoring of species migration and numerous open source software
% projects. E-collaborations allow experts from complementary domains to create close
% collaborations regardless of spatial and temporal distances. E-collaborations
% require cyber-infrastructure that allow members to generated, analyzed, disseminated,
% and consumed information [10]. Open source cyber-infrastructure eliminates
% collaboration hurdles due to software licenses, and can help foster truly
% massive e-collaborations.

% Customized materials design has great potential for impacting virtually all emerging
% technologies, with significant economic consequences [15, 16, 12, 13, 17, 18, 19,
% 20, 21, 22, 23]. However, materials design (including the design of a manufactur-
% ing process route) resulting in the combination of properties desired for a specific
% application is a highly challenging inverse problem due to hierarchical nature of
% materials structure. Material properties are controlled by the hierarchical internal
% structure (over multiple length scales which spans from atomic to macroscopic) as
% well as coupled physical phenomena which can occur at different timescales at each
% of the hierarchical length scales. Characterization of the structure at each of these
% different length scales is often in the form of images which come from different
% experimental/computational techniques resulting in highly heterogeneous data. As a
% result, tailoring the material hierarchical structure to yield desired combinations of
% properties or performance characteristics is enormous difficult.

% There is a critical need for customized analytics that take into account
% the stochastic nature of these data at multiple length scales in order to extract
% relevant and transferable knowledge. Data driven Process-Structure-Property (PSP)
% linkages provide systemic, modular and hierarchical protocols for community engagement
% (i.e., several people making complementary or overlapping contributions to the
% overall curation of materials knowledge). Computationally cheap PSP linkages exact
% valuable materials knowledge from these heterogeneous datasets and provide a
% format that is usable and valuable for design and manufacturing experts.

% The Materials Knowledge Systems in Python project (PyMKS) is the first open
% source materials data science tool that can be used to create high value PSP linkages
% for hierarchical materials that can be leveraged by experts in materials science and
% engineering, manufacturing, and data science communities, and is essential
% component in the cyber-infrastructure needed to realize a modern accelerated
% materials innovation ecosystems. The on going work demonstrates the versatility
% of PyMKS.

% \parttitle{First part title} %if any
% Text for this section.

% \parttitle{Second part title} %if any
% Text for this section.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{TBD}
% \kwd{article}
% \kwd{author}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
%\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
\section{Introduction}

% David - the start is a little weak. Logically, you are better off starting with the need for collaborations - i.e., disciplinary siloed efforts versus cross-disciplinary integrated efforts. Although, the virtues of collaborations have been known for a very long time, there were no mechanisms to scale these up dramatically in intensity and numbers. With the advent of modern information technology - the landscape has changed significantly - with the e-collaborations it is now possible to do this. Discuss the main elements of e-collaborations broadly and point out that open source and open access code repositories are a foundational element for an e-collaboration platform. Talk also about the critical role they play in digital capture, curation, and dissemination of workflows - which is central to community adoption and engagement (e.g., e-Science gateways).




% After this broad introduction, you can then narrow down your focus to code repositories and workflows - talk about the current challenges broadly.

% Then move the discussion to materials science domain where this presents a major new opportunity ...

%% May want to integrate this.
Current practices for developing tools and infrastructure used in multiscale materials design, development, and deployment are generally highly localized (sometimes even within a single organization) resulting in major inefficiencyies (duplication of effort, lack of code review, not engaging the right talent for the right task, etc.). Although it is well known that the pace of discovery and innovation  significantly increases with effective collaboration \cite{sawhney2005collaborating, edwards2009open, bayne2008interdisciplinary, boudreau2010open}, scaling such efforts to large heterogeneous communities such as those engaged in materials innovation has been very difficult.

The advent of information technology has facilitated massive electronic collaborations (generally referred to as e-collaborations) that have lead to significant advances in several domains including the discovery of the Higg's boson \cite{aad2012observation}, the sequencing of the human genome \cite{lander2001initial}, the Polymath project \cite{cranshaw2011polymath}, the monitoring of species migration \cite{dickinson2010citizen, hochachka2012data} and numerous open source software projects. E-collaborations allow experts from complementary domains to create highly productive collaborations that transcend geographical, temporal, cultural, and organizational distances. E-collaborations require a supporting cyber-infrastructure that allows team members to generate, analyze, disseminate, access, and consume information at dramatically increased pace and/or quantity \cite{atkins2003revolutionizing}. A key element of this emerging cyber-infrastructure is open source software as it eliminates collaboration hurdles due to software licenses and can help foster truly massive e-collaborations. In other words, even with collaborations involving proprietary data, open source cyber-infrastructure provide a common language that can facilitate e-collaborations with large numbers of team members (could even become a community effort).

Several recent national and international initiatives \cite{anderson2011report, MGIwhite, MGI2014} have been launched with the premise that the adoption and utilization of modern data science and informatics toolsets offers a new opportunity to accelerate dramatically the design and deployment cycle of new advanced materials in commercial products. More specifically, it has been recognized that innovation cyber-ecosystems are needed to allow experts from the materials science and engineering, design and manufacturing, and data science domains to collaborate effectively. The challenge in integrating these traditionally disconnected communities comes from the vast differences in how knowledge is captured, curated, and disseminated in these communities \cite{kalidindi2015data}. More specifically, knowledge systems in the materials field are rarely captured in a digital form. In order to create a modern materials innovation ecosystem, it is imperative that we design, develop, and launch novel collaboration platforms that allow automated distilling of materials knowledge from large amounts of heterogeneous data acquired through customized protocols that are necessarily diverse (elaborated next). It is also imperative that this curated materials knowledge is presented to the design and manufacturing experts in highly accessible (open) formats.

Customized materials design has great potential for impacting virtually all emerging technologies, with significant economic consequences \cite{ward2012materials, allison2006integrated, MGIwhite, MGI2014, allison2011integrated, olson2000designing, national2008integrated, schmitz2012integrative, robinson2013tms, allisonintegrated, TMSfieldstudy}. However, materials design (including the design of a manufacturing process route) resulting in the combination of properties desired for a specific application is a highly challenging inverse problem due to the hierarchical nature of materials internal structure. Material properties are controlled by the hierarchical internal structure (over multiple length scales which spans from atomic to macroscopic) as well as coupled physical phenomena which can occur at different timescales at each of the hierarchical length scales. Characterization of the structure at each of these different length scales is often in the form of images which come from different experimental/computational techniques resulting in highly heterogeneous data. As a result, tailoring the material hierarchical structure to yield desired combinations of properties or performance characteristics is enormously difficult. Figure \ref{fig:length_scales} provides a collection of materials images depicting material structures at different length scales, which are generally acquired using diverse protocols and are captured in equally diverse formats.

\begin{figure}
    \includegraphics[scale=.23]{fig/lengthScales1.png}
    \caption{Heirarchical Materials structure at multiple length scales
      a). Simulated graphene crystalline structure. b). Simulated fivefold icosahedral Al-Ag quasicrystals. c).High resolution electron microscopy image of delamination cracks in h-BN particles subjected to compressive stress in the (0001) planes (within a silicon nitride particulate-reinforced silicon carbide composite. d). Electron diffraction pattern of an icosahedral Zn-Mg-Ho quasicrystal. e). Cross-polarised light image of spherulites in in poly-3-hydroxy butyrate (PHB) f). Cast iron with magnesium induced spheroidised graphite. g). SEM micrograph of a taffeta textile fragment h). Optical microscopy image of a cross-section of an aluminium casting  i). X-ray tomography image of open cell polyurethane foam. Images courtesy of Core-Materials \cite{coreMaterials}.}
  \label{fig:length_scales}
\end{figure}

While the generation (from experiments and computer simulations) and dissemination of datasets consisting of heterogeneous images are necessary elements in a modern materials innovation ecosystem, there is an equally critical need for customized analytics that take into account the stochastic nature of these data at multiple length scales in order to extract high value, transferable, knowledge. Data-driven Process-Structure-Property (PSP) linkages \cite{kalidindi2015hierarchical} provides a systemic, modular, and hierarchical framework for community engagement (i.e., several people making complementary or overlapping contributions to the overall curation of materials knowledge). Computationally cheap PSP linkages also communicate effectively the curated materials knowledge to design and manufacturing experts in highly accessible formats.

The Materials Knowledge Systems in Python project (PyMKS) is the first open source materials data analytics toolkit that can be used to create high value PSP linkages for hierarchical materials in large scale efforts driven and directed by an entire community of users. In this regard, it could be a foundational element of the cyber-infrastructure needed to realize a modern materials innovation ecosystem.

\section{Current Materials Innovation Ecosystem}


% Traditionally the development materials data analytics toolsets and data repositories are highly localized within a few individual groups resulting in major inefficiency (unnecessary duplication of codes, inadequate verification and validation of multiple instantiations of code, not engaging the right talent for the right task, etc.)

Open access materials databases and computational tools are critical components of the cyber-infrastructure needed to curate materials knowledge through effective e-collaborations \cite{bhat2015strategy}. Several materials science open source computational toolsets and databases have emerged in recent years to help realize the vision outlined in the Materials Genome Initiative (MGI) and the Integrated Computational Materials Engineering (ICME) paradigm \cite{ward2012materials, allison2006integrated, MGIwhite, MGI2014, allison2011integrated, olson2000designing, national2008integrated, schmitz2012integrative, robinson2013tms, allisonintegrated, TMSfieldstudy}. Yet, the creation and adoption of a standard materials taxonomy and database schema has not been established due to the unwieldy size of material descriptors and heterogeneous data. Additionally, the coupled physical phenomena that govern material properties is too complex to model all aspects of a material simultaneously using a single computational tool. Consequently, current practices have resulted in the development of computation tools and databases with a narrow focus on specific length/structure scales, material classes, or properties.

NIST Data Gateway contains over 100 free and paid query-able web-based materials databases. These databases contain atomic structure, thermodynamics, kinetics, fundamental physical constants, x-ray spectroscopy, among other features \cite{NISTgateway}. NIST DSpace provides a curation of links to several materials community databases \cite{nist2015dspace}. NIST Materials Data Curation Systems (MDCS) is a general online database that aims to facilitate the capturing, sharing, and transforming of materials data \cite{NISTDCS}. Open Quantum Materials Database (OQMD) is an open source data repository for phase diagrams and electronic ground states computed using density functional theory \cite{saal2013materials}. MatWeb is a database containing materials properties for over 100,000 materials \cite{matweb}.
Atomic FLOW of Materials Discovery (AFLOW) databases millions of materials and properties and hosts computational tools that can be used for atomic simulations \cite{curtarolo2012aflow}. The Materials Project (and the tool pyMatgen) \cite{ong2013python, jain2013commentary} provides open web-based access to computed information on known and predicted materials as well as analysis tools for electronic band structures. The Knowledgebase of Interatomic Models (OpenKIM) hosts open source tools for potentials for molecular simulation of materials \cite{kim2010project}. PRedictive Integrated Structural Materials Science (PRISMS) hosts a suite of ICME tools and datastorage for the metals community focused on microstructure evolution and mechanical properties \cite{prism}.

SPPARKS Kinetic Monte Carlo Simulator (SPPARKS) is a parallel Monte Carlo code for on-lattice and off-lattice models \cite{plimpton2012spparks}. MOOSE is a parallel computational framework for coupled systems of nonlinear equations \cite{gaston2009moose}. Dream3D is a tool used for synthetic microstructures generation, image processing and mesh creation for finite element \cite{groeber2014dream}.

While there exits a sizable number of standard analytics tools \cite{littell2006sas, seabold2010statsmodels, pedregosa2011scikit, albanese2012mlpy, goodfellow2013pylearn2, mckinney2012python, muller2014pystruct, demvsar2004orange, abadi2016tensorflow, van2014scikit}, none of them are tailored to create PSP linkages from materials structure image data and their associated properties. PyMKS aims to seed and nurture an emergent user group in the materials data analytics for establishing homogenization and localization (PSP) linkages by leveraging open source signal processing and machine learning packages in Python. An overview of the PyMKS project accompanied with several examples is presented here. This paper is a call to others interested in participating in this open science activity.


\section{Theoretical Foundations of Materials Knowledge Systems}


Material properties are controlled by their internal structure and the diverse physical phenomena occurring at multiple time and length scales. Generalized composite theories \cite{hill1963elastic, hashin1983analysis} have been developed for hierarchical materials exhibiting well separated length scales in their internal structure. Generally speaking, these theories either address homogenization (i.e., communication of effective properties associated with the structure at a given length scale to a higher length scale) or localization (i.e., spatiotemporal distribution of the imposed macroscale loading conditions to the lower length scale). Consequently, homogenization and localization are the essential building blocks in communicating the salient information in both directions between hierarchical length/structure scales in multiscale materials modeling. It is also pointed out that localization is significantly more difficult to establish, and implicitly provides a solution to homogenization.

The most sophisticated composite theory available today that explicitly accounts for the full details of the material internal structure (also simply referred as microstructure) comes from the use of perturbation theories and Green's functions \cite{brown1955solid, hill1963elastic, kroner1986statistical, kroner1977bounds, kroner1972statistical, etingof1993representations, adams1998mesostructure, fullwood2008strong, torquato2013random, li2006quantitative, milhans2011prediction, adams2013microstructure, garmestani2001statistical}. In this formalism, one usually arrives at a series expansion for both homogenization and localization, where the individual terms in the series involve convolution integrals with kernels based  on  Green's functions. This series expansion was refined and generalized by Adams and co-workers \cite{adams2005finite, adams2013microstructure, binci2008new} through the introduction of the concept of a microstructure function, which conveniently separates each term in the series into a physics-dependent kernel (based on Green's functions) and a microstructure-dependent function (based on the formalism of n-point spatial correlations \cite{etingof1993representations, adams1998mesostructure, fullwood2008strong, torquato2013random, li2006quantitative, milhans2011prediction}).

 Materials Knowledge Systems (MKS) \cite{landi2010multi, kalidindi2010novel, yabansu2014calibrated, al2012multi, kalidindi2011microstructure, gupta2015structure,  cceccen2014data} complements these sophisticated physics-based materials composite theories with a modern data science approach to create a versatile framework for extracting and curating multiscale PSP linkages. More specifically, MKS employs a discretized version of the composite theories mentioned earlier to gain major computational advantages. As a result, highly adaptable and templatable protocols have been created and used successfully to extract robust and versatile homogenization and localization metamodels with impressive accuracy and broad applicability over large microstructure spaces.

 The MKS framework starts with a discretized description of the microstructure function denoted as $m[s, l]$, where $s$ and $l$ index suitable decompositions of the physical space (occupied by a representative volume element of the microstructure) and the local state space of interest. In most applications, the physical space is simply tessellated into voxels on a regular (uniform) grid. On the other hand, the local states encountered in the description of the microstructure often need a combination of diverse attributes (these might include phase identifiers, lattice orientation, chemical composition, defect types and densities, among others). The introduction of the microstructure function allows a stochastic interpretation of the microstructure, where $m[s, l]$ reflects a probability distribution of the distinct local states in each voxel of the microstructure \cite{niezgoda2013novel, niezgoda2011understanding, qidwai2012estimating,niezgoda2010optimized}. Furthermore, the introduction of the local state space (i.e., the complete set of all potential local states) provides a consolidate variable space for combining the diverse attributes (often a combination of scalar and tensor quantities)  needed to describe the  local states in the material structure.

 As noted earlier, the local state space in most advanced materials is likely to demand sophisticated representations. In prior work \cite{yabansu2014calibrated, yabansu2015representation, brough2016microstructure}, it was found that spectral representations on functions on the local state space offered many advantages both in compact representation as well as in reducing the computational cost. In such cases, $l$ indexes the spectral basis functions employed. The selection of these functions depends on the nature of local state descriptors. Examples of these functions include: (i) the Primitive basis (or indicator functions) used to represent simple tessellation schemes \cite{landi2010multi, kalidindi2010novel, al2012multi, kalidindi2011microstructure, gupta2015structure,  cceccen2014data, niezgoda2013novel, niezgoda2011understanding, cecen2016versatile}, (ii) generalized spherical harmonics used to represent functions over the orientation space \cite{yabansu2014calibrated, yabansu2015representation}, and (iii) Legendre polynomials used to represent functions over the concentration space \cite{brough2016microstructure}.

Comparing different microstructures is quite difficult even after expressing them in convenient discretized descriptions, mainly due to the lack of a reference point or a natural origin for the index $s$ in the tessellation of the microstructure volume. Yet the relative spatial distributions of the local states provide a valuable representation of the microstructure that can be used effectively to quantify the microstructure and compare it with other microstructures in  robust and meaningful ways \cite{niezgoda2011understanding, niezgoda2010optimized, niezgoda2013novel, cceccen2014data, cecen2016versatile}. The lowest order of spatial correlations comes in the form of 2-point statistics and can be computed as a correlation of a microstructure function as
\begin{equation}\label{eq:stats}
    f[l, l'| r] = \frac{1}{\Omega[ r]} \sum_{s} m[s, l] m[s +  r, l']
\end{equation}
\begin{figure}
    \centering
    \includegraphics{fig/stats_micro_example.png}
    \caption{Caption}
    \label{fig:stats}
\end{figure}
where $r$, is a discrete spatial vector within the voxelated domain specified by $s$, $f[l, l'|  r]$ is one set of 2-point statistics for the local stats $l$ and $l'$, and $\Omega[ r]$ is a normalization factor that depends on $ r$ \cite{cecen2016versatile}. The physical interpretation of the 2-point statistics is explained in Fig. \ref{fig:stats} with a highly simplified two-phase microstructure (the two phases are colored white and gray). Using the Primitive basis for both the spatial domain and the local state space, $f[l, l'|  r]$ can be interpreted at the probability of finding local states $l$ and $l'$ at the tail and head of the vector $ r$.

2-Point statistics provide a meaningful representation of the microstructure, but create an extremely large feature space that often contains redundant information. Dimensionality reduction can be used to create low dimensional microstructure descriptors from the sets of spatial correlations (based on different selections of $l$ and $l'$) with principal component analysis (PCA). This dimensionality reduction can be mathematically expressed as
\begin{equation} \label{eq:struc}
    f[ r] \approx \sum_{k} \mu[k] \phi[k,  r] + \overline{f[ r]}
\end{equation}
In Eq. \ref{eq:struc}, $f[r]$ is a feature vector consisting of the selected sets of spatial correlations, and $\mu[k]$ are low dimensional microstructure descriptors or principal component scores (PC scores). $\phi[k, r]$ and $\overline{f[r]}$ are the calibrated principal components (PCs) and the mean values for each feature. The central advantage of this approach is that it has been demonstrated that the individual elements of a large ensemble of microstructures can often be represented to sufficient fidelity with only a handful of PC scores [ADDREFS].

After obtaining the needed dimenisonality reduction in the representation of the material structure, machine learning models can be used to create homogenization or localization PSP linkages of interest. As an example, a generic homogenization linkage can be expressed as
\begin{equation} \label{eq:hom}
    p_{eff} = \mathcal{F}(\mu[k])
\end{equation}
 In Eq. \ref{eq:hom}, $p_{eff}$ is the effective materials response (reflecting an effective property in structure-property linkages or an evolved low dimensional microstructure descriptor in process-structure linkages), and $\mathcal{F}$ is a machine learning function that links $\mu[k]$ to $p_{eff}$.

MKS Localization linkages are significantly more complex than the homogenization linkages. These are usually expressed in the same series forms that are derived in the general composite theories, while employing discretized kernels based on Green's functions \cite{brown1955solid, hill1963elastic, kroner1986statistical, kroner1977bounds, kroner1972statistical, etingof1993representations, adams1998mesostructure, fullwood2008strong, torquato2013random, li2006quantitative, milhans2011prediction, adams2013microstructure, garmestani2001statistical}. Mathematically, the MKS localization linkages are expressed as
\begin{multline}
    \label{eq:series}
    p[s] = \sum_{r, l} \alpha[r, l] m[s - r, l] + \sum_{r, r', l, l'} \alpha[r, r', l, l'] m[s - r, l]m[s-r', l'] + ...
\end{multline}
In Eq. \ref{eq:series}, $p[s]$ is the spatially resolved (localized) response field (could be a response variable such as stress or strain rate, or an evolved microstructure function),  and $\alpha[r, l]$ are the Green's function based discretized influence kernels. These digital kernels are calibrated using regression methods \cite{al2012multi, kalidindi2010novel, landi2010multi, yabansu2014calibrated, yabansu2015representation, brough2016microstructure}.

Figs. \ref{fig:hom} and \ref{fig:loc} provide schematic overviews of the MKS homogenization and localization workflows. THE CURRENT IMAGES ARE JUST PLACE HOLDERS. More detailed explanations on the MKS homogenization and localization linkages can be found in prior literature \cite{landi2010multi, kalidindi2010novel, yabansu2014calibrated, al2012multi, kalidindi2011microstructure, gupta2015structure,  cceccen2014data, niezgoda2013novel, niezgoda2011understanding, cecen2016versatile}.

% \begin{equation}
%     \label{eq:loc}
%     p[s] = \sum_{ r, l} \alpha[r, l] m[s -  r, l]
% \end{equation}



\begin{figure}[h!]
  \caption{\csentence{Localization Figure}
     To Do - Place holder}
    \includegraphics[scale=.18]{fig/localization.png}
  \label{fig:loc}
\end{figure}


\begin{figure}[h!]
  \caption{\csentence{Homogenization Figure}
     To Do - Place holder}
    \includegraphics[scale=.18]{fig/homogenization.png}
  \label{fig:hom}
\end{figure}

% Homogenization linkages are created by taking the ensemble average of Eq. \ref{eq:series}. With the ensemble average, the $N^{th}$ term in the series containing the microstructure function $m[s, l]$ becomes the set of $N$-point statistics. It follows that the linear combinations of these $N$-point statistics provide the necessary structure information to predict effect properties, but present an extremely large feature space with redundant information. Low dimensional linearly independent microstructure descriptors are created using the global linear unsupervised dimensionality reduction technique Principal Component Analysis (PCA). These reduced features as well as effective properties serve as inputs and outputs to calibrate machine learning models for homogenization linkages.


\section{Materials Knowledge Systems in Python}

PyMKS is an object-oriented numerical implementation to the MKS theory
developed in the literature~\cite{kalidindi2010novel}. It provides a
high-level, computational efficient framework to implement data
pipelines for classification, cataloging and quantifying materials
structures for PSP relationships. PyMKS is written in Python, a
natural choice for scientific computing due to its ubiquitous use
among the data science community as well as many other favorable
attributes~\cite{perez2011python}. PyMKS is licensed under the
permissive MIT license \cite{MIT} which allows for unrestricted
distribution in commercial and non-commercial systems.

\subsection{Core Functionality}

PyMKS consists of four main components including a set of tools to
compute 2-point statistics, tools to obtain low-dimensional
microstructure descriptors and tools for both homogenization and
localization. In addition, PyMKS has modules for generating data sets
using conventional numerical simulations and a module for plotting and
visualizing customized for microstructures.

The functions \texttt{autocorrelate}, \texttt{crosscorrelate}, and \texttt{correlate} provies APIs to compute the 2-point statistics for a given microstructure as outlined in Eq. \ref{eq:stats}. The \texttt{MKSStructureAnalysis} class provides access to the objective low dimensional structure descriptors, $\mu[k]$. While the default dimensionality reduction technique is PCA, any model from Scikit-learn that has a \texttt{transform\_fit} method can be used. The \texttt{MKSHomogenizationModel} creates a linkage between $\mu[k]$ and a effective material response, $p_{eff}$ as indicated in Eq. \ref{eq:hom}. The default machine learning function is a polynomial regression, but any estimator from Scikit-learn that has \texttt{fit} and \texttt{predict} methods can be used to predict effective material responses or microstructure classes. The \texttt{MKSLocalizationModel} provides the API to calibrate the first order influence kernels $\alpha[r, l]$ in order to predict local materials responses $p[s]$ as indicated by Eq. \ref{eq:series}. The calibration of the influence kernels is done using the multiple linear regression techniques outlined in previous studies \cite{landi2010multi, kalidindi2010novel, yabansu2014calibrated, brough2016microstructure}. The localization model has \texttt{fit} and \texttt{predict} methods to follow the standard API for an estimator.

Classes in the module \texttt{bases} have methods to discretize the raw structure data and introduce the local state variable $l$. The four basis classes are designed to efficiently discretize different types of local state variables \cite{landi2010multi, kalidindi2010novel, yabansu2014calibrated, al2012multi, kalidindi2011microstructure, gupta2015structure,  cceccen2014data, brough2016microstructure}. The \texttt{PrimitiveBasis} class uses indicator (or hat) functions and it is well suited for microstructures that have discrete local states (e.g., distinct thermodynamic phases encountered in describing the microstructure). The \texttt{LegendreBasis} and \texttt{FourierBasis} create spectral representations of functions defined on nonperiodic and periodic continuous local state spaces, respectively. For example, functions over a range of chemical compositions can be described using \texttt{LegendreBasis}, while functions over orientations in two-dimensional space can be described using \texttt{FourierBasis}. As another option, \texttt{GSHBasis} creates compact spectral representations for functions over lattice orientation space (such as those needed to describe polycrystalline microstructures) \cite{ kalidindi2006spectral, shaffer2010building, knezevic2010deformation, al2010spectral, duvvuru2007application, li2003evolution, li2005texture, li2007processing, li2005processing, creuziger2014crystallographic, sundararaghavan2008multi, sundararaghavan2007linear}.

PyMKS also contains modest data generation tools that are used in both examples and in unit tests, and can be found in the module \texttt{datasets}. The \texttt{MicrostructureGenerator} class creates stochastic microstructures using digital filters. A phase field and finite element simulation are implemented with the \texttt{CahnHilliardSimulation} and \texttt{ElasticFESimulation} classes. The microstructures and simulations can be accessed using the functions which have names starting with \texttt{make}. Lastly PyMKS contains some plotting tools that are used throughout the examples which can be found in the module \texttt{tools}.

\subsection{Code Design}

ADD MORE ABOUT ABSTRACTIONS HERE.

\begin{figure}[h!]
  \caption{\csentence{PyMKS Design}
      AGAIN A PLACE HOLDER - An illustration of the design for PyMKS. The four blocks represent the main functionalists in PyMKS. The modules listed on the far left are exposed in the API  while the other plots are private. The arrows indicate module from
      that is either imported.}
    \includegraphics[scale=.5]{fig/pymksOrganization3.png}
  \label{fig:pymks_design}
\end{figure}


\subsection{Underlying Technologies}

PyMKS is built on highly optimized Python packages NumPy \cite{van2011numpy}, SciPy \cite{jones2014scipy}, and Scikit-learn \cite{pedregosa2011scikit}. NumPy arrays are the primary data structure used throughout PyMKS and provide basic algorithmic functionality. SciPy's signal processing and numerical linear algebra functions are used to calibrate models and generate synthetic data. PyMKS is highly integrated with Scikit-learn and mimics its simple API in order to leverage models for dimensionality reduction, regression, classification, and model selection. In addition, PyMKS uses the Python testing framework nose for unit-tests and doc-tests.

Additional packages that can be used with PyMKS include Simple Finite Element in Python (SfePy) \cite{cimrman2014sfepy}, the python wrapper for the FFTW library (pyFFTW) \cite{frigo1998fftw} and the plotting package Matplotlib \cite{hunter2007matplotlib}. SfePy is used for data generation to simulate the linear elastic response of composite materials, and is not necessary for analysis using external data. PyFFTW is an optional package that can be used to reduce run time of Fast Fourier Transforms (FFTs) through the optimized FFTW library and allows for the computation to be run in parallel with simple high level API arguments. If pyFFTW is not installed, NumPy's \texttt{fft} module is used. Matplotlib is used to generate plots found throughout the examples.

\subsection{Development Practices}

The development team for PyMKS is an open community that uses Github for pull-requests, code review, issue tracking and release management - \\ \url{https://github.com/materialsinnovation/pymks}. Additionally a Google group is used as a public forum to discuss the project development, support and announcements \url{pymks-general@googlegroups.com}.

ADD CONTENT ABOUT TRAVIS CI, etc.

PyMKS follows PEP8 standards and uses pull request to ensure code integrity, and combines overlapping functionality between classes using abstractions. Detailed administrative guidelines are outlined in the \texttt{ADMINISTRATA.md} document on Github, and potential developers are encouraged to follow them.

\section{Examples of Homogenization and Localization with PyMKS}

%DAVE - THE PRESENTATION STYLE OF THE EXAMPLES FEELS VERY ADHOC. CAN YOU LOOK AT OTHER PAPERS OR BOOKS TO SEE WHAT ARE SOME GOOD STYLES TO PRESENT THIS WITHOUT MAKING THIS LOOK TOO ADHOC OR INFORMAL. PERHAPS THERE IS A WAY TO DESCRIBE THE CODING PARTS AS BOXED SELF-STANDING DESCRIPTIONS AND THE TEXT CAN THEN FOCUS MAINLY ON THE LOGIC AND EXPLANATIONS. AS WRITTEN I AM AFRAID THAT THE MATERIALS FOLKS WILL NOT BE ABLE TO FOLLOW WHAT YOU ARE SAYING.


    \subsection{Prediction of Effective Stiffness using Homogenization}\label{effective-stiffness-of-composite-material}

    \subsubsection{Data Generation}\label{data-generation}

A set of periodic microstructures and their volume averaged elastic
stress values $\bar{\sigma}_{xx}$ can be generated by importing the
\texttt{make\_elastic\_stress\_random} function from
\texttt{pymks.datasets}. This function has several arguments.
\texttt{n\_samples} is the number of samples that will be generated,
\texttt{size} specifies the dimensions of the microstructures,
\texttt{grain\_size} controls the effective microstructure feature size,
\texttt{elastic\_modulus} and \texttt{poissons\_ratio} are used to
indicate the material property for each of the phases,
\texttt{macro\_strain} is the value of the applied uniaxial strain, and
the \texttt{seed} can be used to change the the random number generator
seed.

Let's go ahead and create 6 different types of microstructures each with
200 samples with dimensions 21 x 21. Each of the 6 samples will have a
different microstructure feature size. The function will return and the
microstructures and their associated volume averaged stress values.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}elastic\PYZus{}stress\PYZus{}random}

\PY{n}{sample\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{200}
\PY{n}{grain\PYZus{}size} \PY{o}{=} \PY{p}{[}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{]}
\PY{n}{n\PYZus{}samples} \PY{o}{=} \PY{p}{[}\PY{n}{sample\PYZus{}size}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{6}
\PY{n}{elastic\PYZus{}modulus} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{310}\PY{p}{,} \PY{l+m+mi}{200}\PY{p}{)}
\PY{n}{poissons\PYZus{}ratio} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{0.28}\PY{p}{,} \PY{l+m+mf}{0.3}\PY{p}{)}
\PY{n}{macro\PYZus{}strain} \PY{o}{=} \PY{l+m+mf}{0.001}
        \PY{n}{size} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{21}\PY{p}{,} \PY{l+m+mi}{21}\PY{p}{)}

\PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}stress\PYZus{}random}\PY{p}{(}
    \PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,} \PY{n}{grain\PYZus{}size}\PY{o}{=}\PY{n}{grain\PYZus{}size}\PY{p}{,}
    \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,} \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,}
    \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}

\end{Verbatim}


    Lets take a look at 3 types the microstructures to get an idea of
what they look like. We can do this by importing
\texttt{draw\_microstructures}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}microstructures}

\PY{n}{X\PYZus{}examples} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{n}{sample\PYZus{}size}\PY{p}{]}
\PY{n}{draw\PYZus{}microstructures}\PY{p}{(}\PY{n}{X\PYZus{}examples}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_8_0.png}
    \end{center}
    { \hspace*{\fill} \\}


    \subsubsection{Calibration of Homogenization Model}\label{modeling-with-mkshomogenizationmodel}

In order to make an instance of the \texttt{MKSHomogenizationModel}, we
need to pass an instance of a basis class. For this particular example,
there are only 2 discrete
phases, so we will use the \texttt{PrimitiveBasis} from \texttt{pymks.bases}.
We only have two phases denoted by 0 and 1, therefore we have two local
states and our domain is 0 to 1. Let's make an instance of the \texttt{MKSHomgenizationModel}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks} \PY{k+kn}{import} \PY{n}{MKSHomogenizationModel}
\PY{k+kn}{from} \PY{n+nn}{pymks.bases} \PY{k+kn}{import} \PY{n}{PrimitiveBasis}

\PY{n}{p\PYZus{}basis} \PY{o}{=} \PY{n}{PrimitiveBasis}\PY{p}{(}\PY{n}{n\PYZus{}states}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{domain}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
\PY{n}{model} \PY{o}{=} \PY{n}{MKSHomogenizationModel}\PY{p}{(}\PY{n}{basis}\PY{o}{=}\PY{n}{p\PYZus{}basis}\PY{p}{,} \PY{n}{periodic\PYZus{}axes}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}
                               \PY{n}{correlations}\PY{o}{=}\PY{p}{[}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{)}

\end{Verbatim}

The default machine model used to create the homogenization linkage is polynomial regression.
We need to optimize the number of principal components and the degree of polynomial.
To do this we are going to split the data into test and training
sets. This can be done using the
\texttt{train\_test\_spilt} function from \texttt{sklearn}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{sklearn.cross\PYZus{}validation} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}

\PY{n}{flat\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{p}{)} \PY{o}{+} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{size}\PY{p}{,}\PY{p}{)}
\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}
    \PY{n}{X}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{flat\PYZus{}shape}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{,}\PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
\PY{k}{print}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\PY{k}{print}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(960, 441)
(240, 441)

    \end{Verbatim}

We will use cross validation with the testing data to find the optimal value for our model parameters using \texttt{GridSeachCV} from \texttt{sklearn}. We will pass a dictionary \texttt{params\_to\_tune} with the range of the degree of the
polynomial and the number of principal components
we want to search. Let's vary \texttt{n\_components} from 1 to 11 and \texttt{degree} from
1 to 3.
    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{sklearn.grid\PYZus{}search} \PY{k+kn}{import} \PY{n}{GridSearchCV}

\PY{n}{params\PYZus{}to\PYZus{}tune} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,}
                  \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{12}\PY{p}{)}\PY{p}{\PYZcb{}}
\PY{n}{fit\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{size}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{\PYZcb{}}
\PY{n}{gs} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{params\PYZus{}to\PYZus{}tune}\PY{p}{,}
                  \PY{n}{fit\PYZus{}params}\PY{o}{=}\PY{n}{fit\PYZus{}params}\PY{p}{)}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}

\end{Verbatim}

    The default \texttt{score} method for the
\texttt{MKSHomogenizationModel} is R-squared.
% value. Let's look at the how the mean R-squared values and their
% standard deviations change, as we varied the number of
% \texttt{n\_components} and \texttt{degree}, using
% \texttt{draw\_gridscores\_matrix} from \texttt{pymks.tools}.

%     \begin{Verbatim}[commandchars=\\\{\}]

% {\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}gridscores\PYZus{}matrix}

%          \PY{n}{draw\PYZus{}gridscores\PYZus{}matrix}\PY{p}{(}\PY{n}{gs}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{score\PYZus{}label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZhy{}Squared}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
%                                 \PY{n}{param\PYZus{}labels}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of Components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Order of Polynomial}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
% \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_27_0.png}
%     \end{center}
%     { \hspace*{\fill} \\}

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Order of Polynomial}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{degree}\PY{p}{)}
\PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of Components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}\PY{o}{.}\PY{n}{n\PYZus{}components}\PY{p}{)}
\PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZhy{}squared Value}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{gs}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

Order of Polynomial 2
Number of Components 11
R-squared Value 0.999808062591

    \end{Verbatim}

    For the parameter range that we searched, we have found that a model
with 2nd order polynomial and 11 components had the best R-squared
value. Let's look at the results using \texttt{draw\_grid\_scores}, and set the model to have those parameter values.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}gridscores}

\PY{n}{gs\PYZus{}deg\PYZus{}1} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{gs}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}
            \PY{k}{if} \PY{n}{x}\PY{o}{.}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
\PY{n}{gs\PYZus{}deg\PYZus{}2} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{gs}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}
            \PY{k}{if} \PY{n}{x}\PY{o}{.}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
\PY{n}{gs\PYZus{}deg\PYZus{}3} \PY{o}{=} \PY{p}{[}\PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{gs}\PY{o}{.}\PY{n}{grid\PYZus{}scores\PYZus{}}
            \PY{k}{if} \PY{n}{x}\PY{o}{.}\PY{n}{parameters}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}

\PY{n}{draw\PYZus{}gridscores}\PY{p}{(}\PY{p}{[}\PY{n}{gs\PYZus{}deg\PYZus{}1}\PY{p}{,}  \PY{n}{gs\PYZus{}deg\PYZus{}2}\PY{p}{,} \PY{n}{gs\PYZus{}deg\PYZus{}3}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{data\PYZus{}labels}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1st Order}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2nd Order}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3rd Order}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}
                 \PY{n}{param\PYZus{}label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Number of Components}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                 \PY{n}{score\PYZus{}label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZhy{}Squared}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_31_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model} \PY{o}{=} \PY{n}{gs}\PY{o}{.}\PY{n}{best\PYZus{}estimator\PYZus{}}

\end{Verbatim}

Now that we have found optimal values for the parameters \texttt{n\_components} and
\texttt{degree}, lets fit the model with the data.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
\end{Verbatim}

    \subsubsection{Prediction of Effective Stiffness}\label{prediction-using-mkshomogenizationmodel}

    Let's generate some more data to validate our
model. We are going to generate 20 samples of all
six different types of microstructures using the same
\texttt{make\_elastic\_stress\_random} function.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{test\PYZus{}sample\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{20}
\PY{n}{n\PYZus{}samples} \PY{o}{=} \PY{p}{[}\PY{n}{test\PYZus{}sample\PYZus{}size}\PY{p}{]} \PY{o}{*} \PY{l+m+mi}{6}
\PY{n}{X\PYZus{}new}\PY{p}{,} \PY{n}{y\PYZus{}new} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}stress\PYZus{}random}\PY{p}{(}
    \PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,} \PY{n}{grain\PYZus{}size}\PY{o}{=}\PY{n}{grain\PYZus{}size}\PY{p}{,}
    \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,} \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,}
    \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}

\end{Verbatim}

    Now let's predict the stress values for the new microstructures using the \texttt{predict} method.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{y\PYZus{}predict} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}new}\PY{p}{)}

\end{Verbatim}

    We can look to see, if the low-dimensional representation of the new
data is similar to the low-dimensional representation of the data we
used to fit the model using \texttt{draw\_components\_scatter} from
\texttt{pymks.tools}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}components\PYZus{}scatter}

\PY{n}{draw\PYZus{}components\PYZus{}scatter}\PY{p}{(}\PY{p}{[}\PY{n}{model}\PY{o}{.}\PY{n}{reduced\PYZus{}fit\PYZus{}data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}
                         \PY{n}{model}\PY{o}{.}\PY{n}{reduced\PYZus{}predict\PYZus{}data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{]}\PY{p}{,}
                         \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_41_0.png}
    \end{center}
    { \hspace*{\fill} \\}

    The predicted data seems to be reasonably similar to the data we used to
fit the model with. Now let's look at the score value for the predicted
data. We can evaluate our prediction by looking at a
goodness-of-fit plot. We can do this by importing
\texttt{draw\_goodness\_of\_fit} from \texttt{pymks.tools}.

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}goodness\PYZus{}of\PYZus{}fit}

 \PY{n}{fit\PYZus{}data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{y}\PY{p}{,} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{]}\PY{p}{)}
 \PY{n}{pred\PYZus{}data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{y\PYZus{}new}\PY{p}{,} \PY{n}{y\PYZus{}predict}\PY{p}{]}\PY{p}{)}
 \PY{n}{draw\PYZus{}goodness\PYZus{}of\PYZus{}fit}\PY{p}{(}\PY{n}{fit\PYZus{}data}\PY{p}{,} \PY{n}{pred\PYZus{}data}\PY{p}{,}
                      \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{homogenization_stress_2D_files/homogenization_stress_2D_47_0.png}
    \end{center}
    { \hspace*{\fill} \\}

\subsection{Prediction of Local Strain Field with Localization}\label{linear-elasticity-in-2d-for-3-phases}


% \subsubsection{Calibration Data and Delta
% Microstructures}\label{calibration-data-and-delta-microstructures}

% Because we are using distinct phases and the contrast is low we
% only need delta microstructures and their
% strain fields to calibrate the first-order
% influence coefficients.

% Here we use the \texttt{make\_delta\_microstructure} function from
% \texttt{pymks.datasets} to create the delta microstructures needed to
% calibrate the first-order influence coefficients for a two-phase
% microstructure. The \texttt{make\_delta\_microstructure} function uses
% SfePy to generate the data.

%     \begin{Verbatim}[commandchars=\\\{\}]
% {\color{incolor}In [{\color{incolor}1}]:}
%         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
%         \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
% \end{Verbatim}

%     \begin{Verbatim}[commandchars=\\\{\}]
% {\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}microstructures}
%         \PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}delta\PYZus{}microstructures}

%         \PY{n}{n} \PY{o}{=} \PY{l+m+mi}{21}
%         \PY{n}{n\PYZus{}phases} \PY{o}{=} \PY{l+m+mi}{3}
% \end{Verbatim}

%     Let's take a look at a few of the delta microstructures by importing
% \texttt{draw\_microstructures} from \texttt{pymks.tools}.

%     \begin{Verbatim}[commandchars=\\\{\}]
% {\color{incolor}In [{\color{incolor}3}]:} \PY{n}{draw\PYZus{}microstructures}\PY{p}{(}\PY{n}{X\PYZus{}delta}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
% \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_6_0.png}
%     \end{center}
%     { \hspace*{\fill} \\}

%     Using delta microstructures for the calibration of the first-order
% influence coefficients is essentially the same as using a unit
% impulse response to find the kernel of a system in signal processing. Any given delta
% microstructure is composed of only two phases with the center of the microstructure
% and a different phase from the rest. The number
% of delta microstructures that are needed to calibrated the first-order
% coefficients is $N(N-1)$ where $N$ is the number of phases, therefore in
% this example we need 6 delta microstructures.

\subsubsection{Generating Calibration Data}\label{generating-calibration-data}

% The \texttt{make\_elasticFEstrain\_delta} function from
% \texttt{pymks.datasets} provides an easy interface to generate delta
% microstructures and their strain fields, which can then be used for
% calibration of the influence coefficients.

In this example, lets look at a three phase microstructure with elastic
moduli values of 80, 100 and 120 and Poisson's ratio values all equal to
0.3. Let's also set the macroscopic imposed strain equal to 0.02. All of
these parameters used in the simulation must be passed into the
\texttt{make\_elasticFEstrain\_delta}  function from
\texttt{pymks.datasets}. The number of Poisson's
ratio values and elastic moduli values indicates the number of phases.

     \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}microstructures}
\PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}delta\PYZus{}microstructures}

\PY{n}{n} \PY{o}{=} \PY{l+m+mi}{21}
\PY{n}{n\PYZus{}phases} \PY{o}{=} \PY{l+m+mi}{3}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}delta}
\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}microstructure\PYZus{}strain}

\PY{n}{elastic\PYZus{}modulus} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{80}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{120}\PY{p}{)}
\PY{n}{poissons\PYZus{}ratio} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{l+m+mf}{0.3}\PY{p}{)}
\PY{n}{macro\PYZus{}strain} \PY{o}{=} \PY{l+m+mf}{0.02}
\PY{n}{size} \PY{o}{=} \PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{n}\PY{p}{)}

\PY{n}{X\PYZus{}delta}\PY{p}{,} \PY{n}{strains\PYZus{}delta} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}delta}\PY{p}{(}
    \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,}
    \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,}
    \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,} \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{)}

\end{Verbatim}

    Let's take a look at one of the delta microstructures and the
$\varepsilon_{xx}$ strain field.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{draw\PYZus{}microstructure\PYZus{}strain}\PY{p}{(}\PY{n}{X\PYZus{}delta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{strains\PYZus{}delta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_11_0.png}
    \end{center}
    { \hspace*{\fill} \\}

\subsubsection{Calibrating Localization Model}\label{calibrating-first-order-influence-coefficients}

Now that we have the delta microstructures and their strain fields, we
will calibrate the influence coefficients by creating an instance of the
\texttt{MKSLocalizatoinModel} class. Because we are going to calibrate
the influence coefficients with microstructures that contain discrete local
states (in this case phases), we can create an
instance of \texttt{PrimitiveBasis} with \texttt{n\_states} equal to 3,
and use it to create an instance of \texttt{MKSLocalizationModel}.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks} \PY{k+kn}{import} \PY{n}{MKSLocalizationModel}
\PY{k+kn}{from} \PY{n+nn}{pymks} \PY{k+kn}{import} \PY{n}{PrimitiveBasis}

\PY{n}{p\PYZus{}basis} \PY{o}{=}\PY{n}{PrimitiveBasis}\PY{p}{(}\PY{n}{n\PYZus{}states}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{domain}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}
\PY{n}{model} \PY{o}{=} \PY{n}{MKSLocalizationModel}\PY{p}{(}\PY{n}{basis}\PY{o}{=}\PY{n}{p\PYZus{}basis}\PY{p}{)}

\end{Verbatim}

    Now, pass the delta microstructures and their strain fields into the
\texttt{fit} method to calibrate the first-order influence coefficients.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}delta}\PY{p}{,} \PY{n}{strains\PYZus{}delta}\PY{p}{)}

\end{Verbatim}

    That's it, the influence coefficient have been calibrated. Let's take a
look at them.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}coeff}

\PY{n}{draw\PYZus{}coeff}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}

\end{Verbatim}
\begin{center}
\adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_18_0.png}
\end{center}
% { \hspace*{\fill} \\}

\subsubsection{Prediction of the Strain Field for a Random
Microstructure}\label{predict-of-the-strain-field-for-a-random-microstructure}

Let's now use our instance of the \texttt{MKSLocalizationModel} class
with calibrated influence coefficients to compute the strain field for a
random two-phase microstructure and compare it with the results from a
finite element simulation.

The \texttt{make\_elasticFEstrain\_random} function from
\texttt{pymks.datasets} is an easy way to generate a random
microstructure and its strain field results from finite element
analysis.
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{pymks.datasets} \PY{k+kn}{import} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}random}

\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{101}\PY{p}{)}
\PY{n}{X}\PY{p}{,} \PY{n}{strain} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}random}\PY{p}{(}
    \PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,}
    \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,}
    \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{)}
\PY{n}{draw\PYZus{}microstructure\PYZus{}strain}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{,} \PY{n}{strain}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}

\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_21_0.png}
    \end{center}
    { \hspace*{\fill} \\}


Now, to get the strain field from the \texttt{MKSLocalizationModel},
just pass the same microstructure to the \texttt{predict} method.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{n}{strain\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

\end{Verbatim}

    Finally let's compare the results from finite element simulation and the
MKS model.

    \begin{Verbatim}[commandchars=\\\{\}]

\PY{k+kn}{from} \PY{n+nn}{pymks.tools} \PY{k+kn}{import} \PY{n}{draw\PYZus{}strains\PYZus{}compare}

\PY{n}{draw\PYZus{}strains\PYZus{}compare}\PY{p}{(}\PY{n}{strain}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{strain\PYZus{}pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}

\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}

\section{Conclusion}

To Do

% \subsubsection{Resizing the Coefficeints to use on Larger
% Microstructures}\label{resizing-the-coefficeints-to-use-on-larger-microstructures}

% The influence coefficients that were calibrated on a smaller
% microstructure can be used to predict the strain field on a larger
% microstructure though spectral interpolation {[}3{]}, but accuracy of
% the MKS model drops slightly. To demonstrate how this is done, let's
% generate a new larger random microstructure and its strain field.

%     \begin{Verbatim}[commandchars=\\\{\}]

% {\color{incolor}In [{\color{incolor}13}]:} \PY{n}{m} \PY{o}{=} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{n}
%          \PY{n}{size} \PY{o}{=} \PY{p}{(}\PY{n}{m}\PY{p}{,} \PY{n}{m}\PY{p}{)}
%          \PY{k}{print} \PY{n}{size}
%          \PY{n}{X}\PY{p}{,} \PY{n}{strain} \PY{o}{=} \PY{n}{make\PYZus{}elastic\PYZus{}FE\PYZus{}strain\PYZus{}random}\PY{p}{(}\PY{n}{n\PYZus{}samples}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{elastic\PYZus{}modulus}\PY{o}{=}\PY{n}{elastic\PYZus{}modulus}\PY{p}{,}
%                                                   \PY{n}{poissons\PYZus{}ratio}\PY{o}{=}\PY{n}{poissons\PYZus{}ratio}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{size}\PY{p}{,}
%                                                   \PY{n}{macro\PYZus{}strain}\PY{o}{=}\PY{n}{macro\PYZus{}strain}\PY{p}{)}
%          \PY{n}{draw\PYZus{}microstructure\PYZus{}strain}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{p}{,} \PY{n}{strain}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
% \end{Verbatim}

%     \begin{Verbatim}[commandchars=\\\{\}]
% (63, 63)
%     \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_30_1.png}
%     \end{center}
%     { \hspace*{\fill} \\}

%     The influence coefficients that have already been calibrated on a $n$ by
% $n$ delta microstructures, need to be resized to match the shape of the
% new larger $m$ by $m$ microstructure that we want to compute the strain
% field for. This can be done by passing the shape of the new larger
% microstructure into the \texttt{resize\_coeff} method.

%     \begin{Verbatim}[commandchars=\\\{\}]

% {\color{incolor}In [{\color{incolor}14}]:} \PY{n}{model}\PY{o}{.}\PY{n}{resize\PYZus{}coeff}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{)}

% \end{Verbatim}

%     Let's now take a look that ther resized influence coefficients.

%     \begin{Verbatim}[commandchars=\\\{\}]

% {\color{incolor}In [{\color{incolor}15}]:} \PY{n}{draw\PYZus{}coeff}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}
% \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_34_0.png}
%     \end{center}
%     { \hspace*{\fill} \\}

%     Because the coefficients have been resized, they will no longer work for
% our original $n$ by $n$ sized microstructures they were calibrated on,
% but they can now be used on the $m$ by $m$ microstructures. Just like
% before, just pass the microstructure as the argument of the
% \texttt{predict} method to get the strain field.

%     \begin{Verbatim}[commandchars=\\\{\}]
% {\color{incolor}In [{\color{incolor}16}]:} \PY{n}{strain\PYZus{}pred} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}

%          \PY{n}{draw\PYZus{}strains\PYZus{}compare}\PY{p}{(}\PY{n}{strain}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{strain\PYZus{}pred}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
% \end{Verbatim}

%     \begin{center}
%     \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{localization_elasticity_multiphase_2D_files/localization_elasticity_multiphase_2D_36_0.png}
%     \end{center}
%     { \hspace*{\fill} \\}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{backmatter}

% \section*{Competing interests}
%   The authors declare that they have no competing interests.

% \section*{Author's contributions}
%     Text for this section \ldots

% \section*{Acknowledgements}
%   Text for this section \ldots
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file (bmc-mathphys, vancouver, spbasic).
\bibliography{bmc_article}      % Bibliography file (usually '*.bib' )
% for author-year bibliography (bmc-mathphys or spbasic)
% a) write to bib file (bmc-mathphys only)
% @settings{label, options="nameyear"}
% b) uncomment next line
%\nocite{label}

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

% \section*{Figures}
%   \begin{figure}[h!]
%   \caption{\csentence{Sample figure title.}
%       A short description of the figure content
%       should go here.}
%       \end{figure}

% \begin{figure}[h!]
%   \caption{\csentence{Sample figure title.}
%       Figure legend text.}
%       \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
% \section*{Tables}
% \begin{table}[h!]
% \caption{Sample table title. This is where the description of the table should go.}
%       \begin{tabular}{cccc}
%         \hline
%           & B1  &B2   & B3\\ \hline
%         A1 & 0.1 & 0.2 & 0.3\\
%         A2 & ... & ..  & .\\
%         A3 & ..  & .   & .\\ \hline
%       \end{tabular}
% \end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section*{Additional Files}
%   \subsection*{Additional file 1 --- Sample additional file title}
%     Additional file descriptions text (including details of how to
%     view the file, if it is in a non-standard format or the file extension).  This might
%     refer to a multi-page table or a figure.

%   \subsection*{Additional file 2 --- Sample additional file title}
%     Additional file descriptions text.


% \end{backmatter}
\end{document}
